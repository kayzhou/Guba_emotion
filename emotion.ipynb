{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 情感分类\n",
    "\n",
    "停用词\n",
    "\n",
    "加载原始文本，只考虑'1', '2', '3', '4', '-'五类，'x'不确定的暂时不考虑。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded succeed\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from collections import Counter\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import numpy as np\n",
    "# import jieba\n",
    "from thulac import thulac\n",
    "thu = thulac(user_dict='data/emo-words.txt', seg_only=True)\n",
    "\n",
    "from myclf import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_stopword():\n",
    "    \"\"\"\n",
    "    加载停用词集合\n",
    "    \"\"\"\n",
    "    return set(json.load(open('data/stopword-zh.json')))\n",
    "\n",
    "# stop_word = load_stopword()\n",
    "\n",
    "def load_label_sentence():\n",
    "    \"\"\"\n",
    "    加载原始文本\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    in_dir = 'data/labelled'\n",
    "\n",
    "    for in_name in os.listdir(in_dir):\n",
    "        _in = os.path.join(in_dir, in_name)\n",
    "        # print(_in)\n",
    "        for i, line in enumerate(open(_in)):\n",
    "            if line.strip() == '':\n",
    "                continue\n",
    "            label = line.split('\\t')[0]\n",
    "            s= line.split('\\t')[1]\n",
    "            # 1234：四种情绪，-：没有情绪，x：不确定\n",
    "            if label in ['1', '2', '3', '4', '-']:\n",
    "                if label == '-':\n",
    "                    labels.append('0')\n",
    "                else:\n",
    "                    labels.append(label)\n",
    "            sentences.append(s)\n",
    "\n",
    "    return labels, sentences\n",
    "\n",
    "labels, sentences = load_label_sentence()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "信息增益来计算特征词\n",
    "\n",
    "## one-hot表示法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_freq():\n",
    "    \"\"\"\n",
    "    统计高频词汇\n",
    "    \"\"\"\n",
    "    stopwords = load_stopword()\n",
    "    words_freq = {}\n",
    "    words_ci = {} # 出现某个词，是某类的概率，此问题有五类\n",
    "    class_num = 5\n",
    "    labels_num = [0] * class_num\n",
    "    labels, sentences = load_label_sentence()\n",
    "    \n",
    "    for y, s in zip(labels, sentences):\n",
    "        \n",
    "        # 统计每个类别的数量\n",
    "        labels_num[int(y)] += 1\n",
    "        # 分词\n",
    "        for w in thu.cut(s):\n",
    "            w = w[0]\n",
    "            # 停用词等过滤\n",
    "            if w == '' or w in stopwords or w.isdigit():\n",
    "                continue\n",
    "            elif w in words_freq:\n",
    "                words_freq[w] += 1\n",
    "                words_ci[w][int(y)] += 1\n",
    "            else:\n",
    "                words_freq[w] = 1\n",
    "                words_ci[w] = [0] * class_num\n",
    "                words_ci[w][int(y)] += 1\n",
    "    \n",
    "    # 数量转概率\n",
    "    num2pro = lambda nums: [num / sum(nums) for num in nums]\n",
    "    \n",
    "    # 每类上的概率\n",
    "    v_ci = num2pro(labels_num)\n",
    "    \n",
    "    word_gain = {}\n",
    "    for w in words_ci.keys():\n",
    "        word_ci = words_ci[w]\n",
    "        \n",
    "        v_ci_t = num2pro(word_ci) # 句子出现t是Ci类的概率\n",
    "        \n",
    "        non_word_ci = [labels_num[i] - word_ci[i] for i in range(class_num)] # 不是t时候的各类数量\n",
    "        v_ci_non_t = num2pro(non_word_ci) # 句子不出现t是Ci的概率\n",
    "        \n",
    "        pr_t = words_freq[w] / sum(labels_num) # 存在t的概率\n",
    "        \n",
    "        Gt = Info_gain_of_term(v_ci, v_ci_t, v_ci_non_t, pr_t)\n",
    "        \n",
    "        word_gain[w] = Gt\n",
    "        \n",
    "\n",
    "    word_gain = sorted(word_gain.items(), key=lambda d: d[1], reverse=True) \n",
    "    with open('data/word_gain_freq.txt', 'w') as f:\n",
    "        for w, gain in word_gain:\n",
    "            if words_freq[w] >= 5:\n",
    "                print(w, gain, words_freq[w], sep='\\t', file=f)\n",
    "            \n",
    "\n",
    "            \n",
    "def Info_gain_of_term(v_ci, v_ci_t, v_ci_non_t, pr_t):\n",
    "    \"\"\"\n",
    "    计算信息增益，需要每类的概率，句子出现t是Ci类的概率，不出现t是Ci的概率，存在t的概率\n",
    "    \"\"\"\n",
    "    def info_entropy(p):\n",
    "        if p == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return -p * np.log(p)\n",
    "    \n",
    "    gain = 0\n",
    "    for i in range(len(v_ci)):\n",
    "        gain = gain + (info_entropy(v_ci[i]) - pr_t * info_entropy(v_ci_t[i]) - (1 - pr_t) * info_entropy(v_ci_non_t[i]))\n",
    "    return gain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_features_onehot(features_file_name):\n",
    "\n",
    "    def load_word_list(first=2400):\n",
    "        word_list = []\n",
    "        for i, line in enumerate(open('data/word_gain_freq.txt')):\n",
    "            if i >= first:\n",
    "                break\n",
    "            try:\n",
    "                w, gain, freq = line.strip().split('\\t')\n",
    "            except ValueError:\n",
    "                print('读取词向量出错：行 {}'.format(i))\n",
    "            word_list.append(w)\n",
    "        print('词向量大小', len(word_list))\n",
    "        return word_list\n",
    "\n",
    "    word_list = load_word_list()\n",
    "\n",
    "    print('---- 我的词表 ----')\n",
    "    i = 0\n",
    "    with open(features_file_name, 'w') as f:\n",
    "        for y, s in zip(labels, sentences):\n",
    "            i += 1\n",
    "            if not i % 1000:\n",
    "                print('行 ->', i)\n",
    "            vec = np.zeros(len(word_list))\n",
    "            for w in thu.cut(s):\n",
    "                w = w[0]\n",
    "                # print(w)\n",
    "                try:\n",
    "                    _i = word_list.index(w)\n",
    "                    vec[_i] = 1\n",
    "                except ValueError:\n",
    "                    pass\n",
    "\n",
    "            f.write(y + '\\t' + ','.join(['{:.1f}'.format(num) for num in list(vec)]) + '\\n')\n",
    "    print('总行数：', i)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "引入ACL2018词向量（财经方面）\n",
    "\n",
    "因为该文件是按出现次数排序，那么考虑“掐头去尾”\n",
    "\n",
    "停用词要不要去？也是要考虑的，停用词有时候也起到作用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_word_vec():\n",
    "    \"\"\"\n",
    "    加载ACL2018词向量\n",
    "    \"\"\"\n",
    "    word_vec = {}\n",
    "    print('加载词向量中 ...')\n",
    "    for i, line in enumerate(open('data/sgns.financial.word')):\n",
    "        if i <= 10:\n",
    "            continue\n",
    "        if i > 150000:\n",
    "            break\n",
    "        words = line.strip().split(' ')\n",
    "        word = words[0]\n",
    "        word_vec[word] = np.array([float(num) for num in words[1:]])\n",
    "#         except UnicodeDecodeError:\n",
    "#             print(\"编码问题，行 {}\".format(i))\n",
    "    print('加载词完成！一共 {}个词'.format(len(word_vec)))\n",
    "    return word_vec\n",
    "\n",
    "\n",
    "def make_features_ACLwv(features_file_name):\n",
    "    word_vec = load_word_vec()\n",
    "    i = 0\n",
    "    # 建立训练文件：ACL的wv\n",
    "    print('---- ACL wv ----')\n",
    "    with open(features_file_name, 'w') as f:\n",
    "        for y, s in zip(labels, sentences):\n",
    "            i += 1\n",
    "            if not i % 1000:\n",
    "                print('行 -> {}'.format(i))\n",
    "            count = 0\n",
    "            vec = np.zeros(300)\n",
    "\n",
    "            for w in thu.cut(s): # 对分词结果进行处理\n",
    "                w = w[0]\n",
    "    #             if w in stop_word:\n",
    "    #                 continue\n",
    "                if w in word_vec:\n",
    "                    vec += word_vec[w]\n",
    "                    count += 1\n",
    "\n",
    "            if count != 0:\n",
    "                vec = vec / count\n",
    "\n",
    "    #         if count > 0:\n",
    "            f.write(y + '\\t' + ','.join(['{:.6f}'.format(num) for num in list(vec)]) + '\\n')\n",
    "    print('总行数：', i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "mywv_model = Word2Vec.load(\"model/guba_word2vec.model\")\n",
    "\n",
    "def make_features_mywv(features_file_name):\n",
    "    i = 0\n",
    "    # 建立训练文件: 我的wv\n",
    "    print('---- 我的wv ----')\n",
    "    with open(features_file_name, 'w') as f:\n",
    "        for y, s in zip(labels, sentences):\n",
    "            i += 1\n",
    "            if not i % 1000:\n",
    "                print('行 -> {}'.format(i))\n",
    "            count = 0\n",
    "            vec = np.zeros(300)\n",
    "\n",
    "            for w in thu.cut(s): # 对分词结果进行处理\n",
    "                w = w[0]\n",
    "                if w in mywv_model.wv:\n",
    "                    vec += mywv_model.wv[w]\n",
    "                    count += 1\n",
    "\n",
    "            if count != 0:\n",
    "                vec = vec / count\n",
    "\n",
    "    #         if count > 0:\n",
    "            f.write(y + '\\t' + ','.join(['{:.6f}'.format(num) for num in list(vec)]) + '\\n')\n",
    "    print('总行数：', i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ~ ⬆️准备训练数据 ⬇️开始训练\n",
    "\n",
    "机器学习算法包括：KNN、LR、随机森林、决策树、GBDT、SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "读取词向量出错：行 453\n",
      "读取词向量出错：行 1248\n",
      "读取词向量出错：行 1249\n",
      "词向量大小 2400\n",
      "---- 我的词表 ----\n",
      "行 -> 1000\n",
      "行 -> 2000\n",
      "行 -> 3000\n",
      "行 -> 4000\n",
      "行 -> 5000\n",
      "行 -> 6000\n",
      "行 -> 7000\n",
      "行 -> 8000\n",
      "行 -> 9000\n",
      "行 -> 10000\n",
      "行 -> 11000\n",
      "总行数： 11968\n",
      "加载词向量中 ...\n",
      "加载词完成！一共 149960个词\n",
      "---- ACL wv ----\n",
      "行 -> 1000\n",
      "行 -> 2000\n",
      "行 -> 3000\n",
      "行 -> 4000\n",
      "行 -> 5000\n",
      "行 -> 6000\n",
      "行 -> 7000\n",
      "行 -> 8000\n",
      "行 -> 9000\n",
      "行 -> 10000\n",
      "行 -> 11000\n",
      "总行数： 11968\n",
      "---- 我的wv ----\n",
      "行 -> 1000\n",
      "行 -> 2000\n",
      "行 -> 3000\n",
      "行 -> 4000\n",
      "行 -> 5000\n",
      "行 -> 6000\n",
      "行 -> 7000\n",
      "行 -> 8000\n",
      "行 -> 9000\n",
      "行 -> 10000\n",
      "行 -> 11000\n",
      "总行数： 11968\n"
     ]
    }
   ],
   "source": [
    "get_word_freq() # 词分析\n",
    "make_features_onehot('data/train/onehot.txt')\n",
    "make_features_ACLwv('data/train/ACLwv.txt')\n",
    "make_features_mywv('data/train/mywv.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_data(in_name, num=-1):\n",
    "    \"\"\"\n",
    "    加载训练数据\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "    for line in open(in_name):\n",
    "\n",
    "            \n",
    "            \n",
    "        label, vec = line.strip().split('\\t')\n",
    "        \n",
    "        if num == 0: # 是否有情绪\n",
    "            if label == '0':\n",
    "                label = 0\n",
    "            else:\n",
    "                label = 1\n",
    "        \n",
    "        if num == 2: # 正负情绪\n",
    "            if label == '2':\n",
    "                label = 1\n",
    "            elif label == '0':\n",
    "                continue\n",
    "            else:\n",
    "                label = 0\n",
    "                \n",
    "        if num == 4: # 四种情绪\n",
    "            if label == '0':\n",
    "                continue\n",
    "                \n",
    "        x = np.array([float(v) for v in vec.split(',')])\n",
    "        y.append(label)\n",
    "        X.append(x)\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8246, 3000) (8246,)\n",
      "******************* LR ********************\n",
      "accuracy of CV: 0.6672339459345854\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.73      0.89      0.80      1197\n",
      "          1       0.33      0.15      0.21       453\n",
      "\n",
      "avg / total       0.62      0.68      0.64      1650\n",
      "\n",
      "******************* GBDT ********************\n"
     ]
    }
   ],
   "source": [
    "def stack_X_y(X1, y1, X2, y2, out_name=0):\n",
    "    print(X1.shape, y1.shape, X2.shape, y2.shape)\n",
    "    if len(y1) != len(y2):\n",
    "        print('两列表长度不同，不同合并。')\n",
    "        return -1\n",
    "    _len = len(X1)\n",
    "    X = []\n",
    "    for i in range(_len):\n",
    "        xi= np.hstack([X1[i], X2[i]])\n",
    "        X.append(xi)\n",
    "    X = np.array(X)\n",
    "    y = np.array(y1)\n",
    "\n",
    "    if out_name != 0:\n",
    "        with open(out_name, 'w') as f:\n",
    "            for xi, yi in zip(X, y):\n",
    "                f.write(yi + '\\t' + ','.join(['{:.6f}'.format(num) for num in list(xi)]) + '\\n')\n",
    "    print('合并数据完成。')\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def train():\n",
    "    \"\"\"\n",
    "    调参\n",
    "    \"\"\"\n",
    "    # 合并数据\n",
    "#     X1, y1 = load_train_data('data/train/onehot.txt')\n",
    "#     X2, y2 = load_train_data('data/train/ACLwv.txt')\n",
    "#     X1, y1 = stack_X_y(X1, y1, X2, y2)\n",
    "#     X3, y3 = load_train_data('data/train/mywv.txt')\n",
    "#     X, y = stack_X_y(X1, y1, X3, y3, out_name='data/train/all-180912.txt')\n",
    "    \n",
    "    X, y = load_train_data('data/train/all-180912.txt', num=2)\n",
    "    print(X.shape, y.shape)\n",
    "\n",
    "    # 划分数据集\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=23)\n",
    "\n",
    "    # 初始化分类器\n",
    "    test_classifiers = ['LR', 'GBDT']\n",
    "    classifiers = {\n",
    "        'NB':naive_bayes_classifier,\n",
    "        'KNN':knn_classifier,\n",
    "        'LR':logistic_regression_classifier,\n",
    "        'RF':random_forest_classifier,\n",
    "        'DT':decision_tree_classifier,\n",
    "        'SVM':svm_classifier,\n",
    "        'SVMCV':svm_cross_validation,\n",
    "        'GBDT':gradient_boosting_classifier\n",
    "    }\n",
    "\n",
    "\n",
    "    for classifier in test_classifiers:\n",
    "        print('******************* {} ********************'.format(classifier))\n",
    "        if classifier == \"GBDT\":\n",
    "            clf = GradientBoostingClassifier(learning_rate=0.1, max_depth=5)\n",
    "            clf.fit(X_train, y_train)\n",
    "        if classifier == \"LR\":\n",
    "            clf = LogisticRegression()\n",
    "            clf.fit(X_train, y_train)\n",
    "        else:\n",
    "            clf = classifiers[classifier](X_train, y_train)\n",
    "        evaluate(clf, X, y, X_test, y_test)\n",
    "\n",
    "\n",
    "    # SVR\n",
    "    original_params = {}\n",
    "    for i, setting in enumerate([{'C':0.125}, {'C': 0.25}, {'C':0.5}, {'C':1.0}]):\n",
    "        print('******************* SVR-{} ********************'.format(i))\n",
    "        print(setting)\n",
    "        params = dict(original_params)\n",
    "        params.update(setting)\n",
    "\n",
    "        clf = LinearSVC(**params)\n",
    "        clf.fit(X_train, y_train)\n",
    "        evaluate(clf, X, y, X_test, y_test)\n",
    "\n",
    "    # GBDT\n",
    "    original_params = {'n_estimators': 1000, 'max_leaf_nodes': 4, 'max_depth': 3, 'random_state': 23,\n",
    "                    'min_samples_split': 5}\n",
    "\n",
    "    for i, setting in enumerate([{'learning_rate': 1.0, 'subsample': 1.0},\n",
    "                    {'learning_rate': 0.1, 'subsample': 1.0},\n",
    "                    {'learning_rate': 1.0, 'subsample': 0.5},\n",
    "                    {'learning_rate': 0.1, 'subsample': 0.5},\n",
    "                    {'learning_rate': 0.1, 'max_features': 2}]):\n",
    "        print('******************* GBDT{} ********************'.format(i))\n",
    "        print(setting)\n",
    "        params = dict(original_params)\n",
    "        params.update(setting)\n",
    "\n",
    "        clf = GradientBoostingClassifier(**params)\n",
    "        clf.fit(X_train, y_train)\n",
    "        evaluate(clf, X, y, X_test, y_test)\n",
    "\n",
    "    original_params = {}\n",
    "\n",
    "    \n",
    "def evaluate(clf, X, y, X_test, y_test):\n",
    "    # CV\n",
    "    print('accuracy of CV:', cross_val_score(clf, X, y, cv=5).mean())\n",
    "\n",
    "    # 模型评估\n",
    "    y_pred = []\n",
    "    for i in range(len(X_test)):\n",
    "        y_hat = clf.predict(X_test[i].reshape(1, -1))\n",
    "        y_pred.append(y_hat[0])\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    \n",
    "def train_model():\n",
    "    X, y = load_train_data('data/train/train_data_ACL-20180712.txt')\n",
    "    clf = LogisticRegression(penalty='l2')\n",
    "    print(X.shape, y.shape)\n",
    "    clf.fit(X, y)\n",
    "    # 保存模型\n",
    "    joblib.dump(clf, \"emo-LR-v1.model\")\n",
    "    \n",
    "    \n",
    "train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
