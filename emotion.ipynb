{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 情感分类\n",
    "\n",
    "停用词\n",
    "\n",
    "加载原始文本，只考虑'1', '2', '3', '4', '-'五类，'x'不确定的暂时不考虑。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded succeed\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import glob\n",
    "from collections import Counter\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import numpy as np\n",
    "# import jieba\n",
    "from thulac import thulac\n",
    "thu = thulac(seg_only=True)\n",
    "\n",
    "def load_stopword():\n",
    "    \"\"\"\n",
    "    加载停用词集合\n",
    "    \"\"\"\n",
    "    return set(json.load(open('data/stopword-zh.json')))\n",
    "\n",
    "stop_word = load_stopword()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_label_sentence():\n",
    "    \"\"\"\n",
    "    加载原始文本\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    in_dir = 'data/labelled_split/'\n",
    "    for in_name in glob.glob(in_dir + '*.txt'):\n",
    "        for i, line in enumerate(open(in_name)):\n",
    "            if line.strip() == '': continue\n",
    "            label = line.split('\\t')[0]\n",
    "            s= line.split('\\t')[1]\n",
    "            # 1234：四种情绪，-：没有情绪，x：不确定\n",
    "            if label in ['1', '2', '3', '4', '-']:\n",
    "                if label == '-' or label == 'x':\n",
    "                    labels.append('0')\n",
    "                else:\n",
    "                    labels.append(label)\n",
    "            sentences.append(s)\n",
    "\n",
    "    return labels, sentences\n",
    "\n",
    "\n",
    "labels, sentences = load_label_sentence()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 信息增益 and one-hot\n",
    "\n",
    "~来计算特征词\n",
    "\n",
    "one-hot表示法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "读取词向量出错：行 1012\n",
      "词向量大小 2000\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n"
     ]
    }
   ],
   "source": [
    "def get_word_freq():\n",
    "    \"\"\"\n",
    "    统计高频词汇\n",
    "    \"\"\"\n",
    "    stopwords = load_stopword()\n",
    "    words_freq = {}\n",
    "    words_ci = {} # 出现某个词，是某类的概率，此问题有五类\n",
    "    class_num = 5\n",
    "    labels_num = [0] * class_num\n",
    "    labels, sentences = load_label_sentence()\n",
    "    \n",
    "    for y, s in zip(labels, sentences):\n",
    "        \n",
    "        # 统计每个类别的数量\n",
    "        labels_num[int(y)] += 1\n",
    "        # 分词\n",
    "        for w in thu.cut(s):\n",
    "            w = w[0]\n",
    "            # 停用词等过滤\n",
    "            if w == '' or w in stopwords or w.isdigit():\n",
    "                continue\n",
    "            elif w in words_freq:\n",
    "                words_freq[w] += 1\n",
    "                words_ci[w][int(y)] += 1\n",
    "            else:\n",
    "                words_freq[w] = 1\n",
    "                words_ci[w] = [0] * class_num\n",
    "                words_ci[w][int(y)] += 1\n",
    "    \n",
    "    # 数量转概率\n",
    "    num2pro = lambda nums: [num / sum(nums) for num in nums]\n",
    "    \n",
    "    # 每类上的概率\n",
    "    v_ci = num2pro(labels_num)\n",
    "    \n",
    "    word_gain = {}\n",
    "    for w in words_ci.keys():\n",
    "        word_ci = words_ci[w]\n",
    "        \n",
    "        v_ci_t = num2pro(word_ci) # 句子出现t是Ci类的概率\n",
    "        \n",
    "        non_word_ci = [labels_num[i] - word_ci[i] for i in range(class_num)] # 不是t时候的各类数量\n",
    "        v_ci_non_t = num2pro(non_word_ci) # 句子不出现t是Ci的概率\n",
    "        \n",
    "        pr_t = words_freq[w] / sum(labels_num) # 存在t的概率\n",
    "        \n",
    "        Gt = Info_gain_of_term(v_ci, v_ci_t, v_ci_non_t, pr_t)\n",
    "        \n",
    "        word_gain[w] = Gt\n",
    "        \n",
    "\n",
    "    word_gain = sorted(word_gain.items(), key=lambda d: d[1], reverse=True) \n",
    "    with open('data/word_gain_freq.txt', 'w') as f:\n",
    "        for w, gain in word_gain:\n",
    "            if words_freq[w] >= 5:\n",
    "                print(w, gain, words_freq[w], sep='\\t', file=f)\n",
    "            \n",
    "\n",
    "            \n",
    "def Info_gain_of_term(v_ci, v_ci_t, v_ci_non_t, pr_t):\n",
    "    \"\"\"\n",
    "    计算信息增益，需要每类的概率，句子出现t是Ci类的概率，不出现t是Ci的概率，存在t的概率\n",
    "    \"\"\"\n",
    "    def info_entropy(p):\n",
    "        if p == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return -p * np.log(p)\n",
    "    \n",
    "    gain = 0\n",
    "    for i in range(len(v_ci)):\n",
    "        gain = gain + (info_entropy(v_ci[i]) - pr_t * info_entropy(v_ci_t[i]) - (1 - pr_t) * info_entropy(v_ci_non_t[i]))\n",
    "    return gain\n",
    "    \n",
    "\n",
    "def word_2_vec_one_hot():\n",
    "\n",
    "    def load_word_list(first=2000):\n",
    "        word_list = []\n",
    "        for i, line in enumerate(open('data/word_gain_freq.txt')):\n",
    "            if i >= first:\n",
    "                break\n",
    "            try:\n",
    "                w, gain, freq = line.strip().split('\\t')\n",
    "            except ValueError:\n",
    "                print('读取词向量出错：行 {}'.format(i))\n",
    "            word_list.append(w)\n",
    "        print('词向量大小', len(word_list))\n",
    "        return word_list\n",
    "\n",
    "    word_list = load_word_list()\n",
    "    labels, sentences = load_label_sentence()\n",
    "    i = 0\n",
    "    for y, s in zip(labels, sentences):\n",
    "        i += 1\n",
    "        if not i % 1000:\n",
    "            print(i)\n",
    "        vec = np.zeros(len(word_list))\n",
    "        for w in thu.cut(s):\n",
    "            w = w[0]\n",
    "            # print(w)\n",
    "            try:\n",
    "                _i = word_list.index(w)\n",
    "                vec[_i] = 1\n",
    "            except ValueError:\n",
    "                pass\n",
    "\n",
    "        print(y, ','.join(['{:.1f}'.format(num) for num in list(vec)]), sep='\\t', file=open('train_data_one_hot-20180710.txt', 'a'))\n",
    "    \n",
    "# one-hot \n",
    "get_word_freq() # 词分析\n",
    "word_2_vec_one_hot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 创建训练数据（word vector）\n",
    "\n",
    "引入ACL2018词向量（财经方面）\n",
    "\n",
    "因为该文件是按出现次数排序，那么考虑“掐头去尾”\n",
    "\n",
    "停用词要不要去？也是要考虑的，停用词有时候也起到作用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载词向量中 ...\n",
      "加载词完成！一共 199937个词\n",
      "Counter({'0': 3441, '3': 2600, '2': 2397, '1': 2357, '4': 1130})\n"
     ]
    }
   ],
   "source": [
    "def load_word_vec():\n",
    "    \"\"\"\n",
    "    加载ACL2018词向量，只加载信息增益筛选过后的词\n",
    "    \"\"\"\n",
    "    word_vec = {}\n",
    "    print('加载词向量中 ...')\n",
    "    for i, line in enumerate(open('data/sgns.financial.word')):\n",
    "        if i <= 10:\n",
    "            continue\n",
    "        if i > 200000:\n",
    "            break\n",
    "        words = line.strip().split(' ')\n",
    "        word = words[0]\n",
    "        word_vec[word] = np.array([float(num) for num in words[1:]])\n",
    "#         except UnicodeDecodeError:\n",
    "#             print(\"编码问题，行 {}\".format(i))\n",
    "    print('加载词完成！一共 {}个词'.format(len(word_vec)))\n",
    "    return word_vec\n",
    "\n",
    "\n",
    "word_vec = load_word_vec()\n",
    "cnt = Counter(labels)\n",
    "print(cnt)  \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "行 -> 1000\n",
      "行 -> 2000\n",
      "行 -> 3000\n",
      "行 -> 4000\n",
      "行 -> 5000\n",
      "行 -> 6000\n",
      "行 -> 7000\n",
      "行 -> 8000\n",
      "行 -> 9000\n",
      "行 -> 10000\n",
      "行 -> 11000\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "\n",
    "# 建立训练文件\n",
    "with open('data/train/data-180904.txt', 'w') as f:\n",
    "    for y, s in zip(labels, sentences):\n",
    "        i += 1\n",
    "        if not i % 1000:\n",
    "            print('行 -> {}'.format(i))\n",
    "        count = 0\n",
    "        vec = np.zeros(300)\n",
    "        \n",
    "        for w in thu.cut(s): # 对分词结果进行处理\n",
    "            w = w[0]\n",
    "#             if w in stop_word:\n",
    "#                 continue\n",
    "            if w in word_vec:\n",
    "                vec += word_vec[w]\n",
    "                count += 1\n",
    "        \n",
    "#         for w in jieba.cut(s): # 对分词结果进行处理\n",
    "#             if w in stop_word:\n",
    "#                 continue\n",
    "#             if w in word_vec:\n",
    "#                 vec += word_vec[w]\n",
    "#                 count += 1\n",
    "\n",
    "        if count != 0:\n",
    "            vec = vec / count\n",
    "            \n",
    "        if count > 0:\n",
    "            f.write(y + '\\t' + ','.join(['{:.6f}'.format(num) for num in list(vec)]) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ~ ⬆️准备训练数据 ⬇️开始训练\n",
    "\n",
    "机器学习算法包括：KNN、LR、随机森林、决策树、GBDT、SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier  \n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression  \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Multinomial Naive Bayes Classifier  \n",
    "def naive_bayes_classifier(train_x, train_y):  \n",
    "    from sklearn.naive_bayes import MultinomialNB  \n",
    "    model = MultinomialNB(alpha=0.01)  \n",
    "    model.fit(train_x, train_y)  \n",
    "    return model  \n",
    "  \n",
    "  \n",
    "# KNN Classifier  \n",
    "def knn_classifier(train_x, train_y):  \n",
    "    from sklearn.neighbors import KNeighborsClassifier  \n",
    "    model = KNeighborsClassifier()  \n",
    "    model.fit(train_x, train_y)  \n",
    "    return model  \n",
    "  \n",
    "  \n",
    "# Logistic Regression Classifier  \n",
    "def logistic_regression_classifier(train_x, train_y):  \n",
    "    from sklearn.linear_model import LogisticRegression  \n",
    "    model = LogisticRegression(penalty='l2')  \n",
    "    model.fit(train_x, train_y)  \n",
    "    return model  \n",
    "  \n",
    "  \n",
    "# Random Forest Classifier  \n",
    "def random_forest_classifier(train_x, train_y):  \n",
    "    from sklearn.ensemble import RandomForestClassifier  \n",
    "    model = RandomForestClassifier(n_estimators=8)  \n",
    "    model.fit(train_x, train_y)  \n",
    "    return model  \n",
    "  \n",
    "  \n",
    "# Decision Tree Classifier  \n",
    "def decision_tree_classifier(train_x, train_y):  \n",
    "    from sklearn import tree  \n",
    "    model = tree.DecisionTreeClassifier()  \n",
    "    model.fit(train_x, train_y)  \n",
    "    return model  \n",
    "  \n",
    "  \n",
    "# GBDT(Gradient Boosting Decision Tree) Classifier  \n",
    "def gradient_boosting_classifier(train_x, train_y):  \n",
    "    from sklearn.ensemble import GradientBoostingClassifier  \n",
    "    model = GradientBoostingClassifier(n_estimators=200)  \n",
    "    model.fit(train_x, train_y)  \n",
    "    return model  \n",
    "  \n",
    "  \n",
    "# SVM Classifier  \n",
    "def svm_classifier(train_x, train_y):  \n",
    "    from sklearn.svm import SVC  \n",
    "    model = SVC(kernel='rbf', probability=True)  \n",
    "    model.fit(train_x, train_y)  \n",
    "    return model  \n",
    "  \n",
    "# SVM Classifier using cross validation  \n",
    "def svm_cross_validation(train_x, train_y):  \n",
    "    from sklearn.grid_search import GridSearchCV  \n",
    "    from sklearn.svm import SVC  \n",
    "    model = SVC(kernel='rbf', probability=True)  \n",
    "    param_grid = {'C': [1e-3, 1e-2, 1e-1, 1, 10, 100, 1000], 'gamma': [0.001, 0.0001]}  \n",
    "    grid_search = GridSearchCV(model, param_grid, n_jobs = 1, verbose=1)  \n",
    "    grid_search.fit(train_x, train_y)  \n",
    "    best_parameters = grid_search.best_estimator_.get_params()  \n",
    "    for para, val in list(best_parameters.items()):  \n",
    "        print(para, val)  \n",
    "    model = SVC(kernel='rbf', C=best_parameters['C'], gamma=best_parameters['gamma'], probability=True)  \n",
    "    model.fit(train_x, train_y)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8537, 300) (8537,)\n",
      "******************* KNN ********************\n",
      "accuracy of CV: 0.3994352645748854\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.36      0.55      0.44       687\n",
      "          2       0.50      0.51      0.51       722\n",
      "          3       0.42      0.31      0.35       785\n",
      "          4       0.34      0.18      0.23       368\n",
      "\n",
      "avg / total       0.41      0.41      0.40      2562\n",
      "\n",
      "******************* LR ********************\n",
      "accuracy of CV: 0.5091984544386139\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.49      0.52      0.50       687\n",
      "          2       0.61      0.70      0.65       722\n",
      "          3       0.46      0.52      0.49       785\n",
      "          4       0.44      0.15      0.22       368\n",
      "\n",
      "avg / total       0.51      0.52      0.50      2562\n",
      "\n",
      "******************* RF ********************\n",
      "accuracy of CV: 0.37402356528931935\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.36      0.46      0.40       687\n",
      "          2       0.45      0.50      0.47       722\n",
      "          3       0.35      0.33      0.34       785\n",
      "          4       0.27      0.10      0.15       368\n",
      "\n",
      "avg / total       0.37      0.38      0.37      2562\n",
      "\n",
      "******************* DT ********************\n",
      "accuracy of CV: 0.32622524871966724\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.35      0.35      0.35       687\n",
      "          2       0.41      0.39      0.40       722\n",
      "          3       0.36      0.37      0.36       785\n",
      "          4       0.22      0.23      0.22       368\n",
      "\n",
      "avg / total       0.35      0.35      0.35      2562\n",
      "\n",
      "******************* GBDT ********************\n",
      "accuracy of CV: 0.49631349618742504\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.50      0.52      0.51       687\n",
      "          2       0.60      0.66      0.63       722\n",
      "          3       0.47      0.52      0.49       785\n",
      "          4       0.49      0.25      0.33       368\n",
      "\n",
      "avg / total       0.52      0.52      0.51      2562\n",
      "\n",
      "******************* SVM ********************\n",
      "accuracy of CV: 0.40436180579329656\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.71      0.07      0.12       687\n",
      "          2       0.56      0.51      0.54       722\n",
      "          3       0.35      0.82      0.49       785\n",
      "          4       0.00      0.00      0.00       368\n",
      "\n",
      "avg / total       0.46      0.41      0.33      2562\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kayzhou/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "def load_train_data(in_name, num=5):\n",
    "    \"\"\"\n",
    "    加载训练数据\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "    for line in open(in_name):\n",
    "        label, vec = line.strip().split('\\t')\n",
    "        # 高兴\n",
    "        if num == 3:\n",
    "            if label == '2':\n",
    "                label = '1'\n",
    "            # 没有情绪\n",
    "            elif label == '0':\n",
    "                label = '0'\n",
    "            # 负面\n",
    "            else:\n",
    "                label = '-1'\n",
    "        elif num == 4:\n",
    "            if label == '0':\n",
    "                continue\n",
    "            \n",
    "        x = np.array([float(v) for v in vec.split(',')])\n",
    "        y.append(label)\n",
    "        X.append(x)\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def train():\n",
    "    \"\"\"\n",
    "    调参\n",
    "    \"\"\"\n",
    "    X, y = load_train_data('data/train/data-180904.txt', num=4)\n",
    "    # X, y = load_train_data('data/train/train_data_one_hot-20180710.txt')\n",
    "    print(X.shape, y.shape)\n",
    "\n",
    "    # 划分数据集\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=41)\n",
    "\n",
    "        \n",
    "    # 初始化分类器\n",
    "    test_classifiers = ['KNN', 'LR', 'RF', 'DT', 'GBDT', 'SVM']  \n",
    "    classifiers = {'NB':naive_bayes_classifier,   \n",
    "                  'KNN':knn_classifier,  \n",
    "                   'LR':logistic_regression_classifier,  \n",
    "                   'RF':random_forest_classifier,  \n",
    "                   'DT':decision_tree_classifier,  \n",
    "                  'SVM':svm_classifier,  \n",
    "                'SVMCV':svm_cross_validation,  \n",
    "                 'GBDT':gradient_boosting_classifier  \n",
    "    } \n",
    "    \n",
    "    for classifier in test_classifiers:  \n",
    "        print('******************* {} ********************'.format(classifier))\n",
    "        clf = classifiers[classifier](X_train, y_train)\n",
    "\n",
    "        # CV\n",
    "        print('accuracy of CV:', cross_val_score(clf, X, y, cv=5).mean())\n",
    "\n",
    "#         # 执行训练\n",
    "#         clf.fit(X_train, y_train)\n",
    "\n",
    "        # 模型评估\n",
    "        y_pred = []\n",
    "        for i in range(len(X_test)):\n",
    "            y_hat = clf.predict(X_test[i].reshape(1, -1))\n",
    "            y_pred.append(y_hat[0])\n",
    "        print(classification_report(y_test, y_pred))\n",
    "\n",
    "\n",
    "def train_model():\n",
    "    X, y = load_train_data('data/train/train_data_ACL-20180712.txt')\n",
    "    clf = LogisticRegression(penalty='l2')\n",
    "    print(X.shape, y.shape)\n",
    "    clf.fit(X, y)\n",
    "    # 保存模型\n",
    "    joblib.dump(clf, \"emo-LR-v1.model\")\n",
    "\n",
    "train()\n",
    "# train_model()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "------------- ～ 华丽丽的分割线 ～ ---------------\n",
    "\n",
    "## LSTM（注意已经切到lstm.ipynb）\n",
    "\n",
    "### 为lstm做准备，训练数据\n",
    "\n",
    "句子不必对齐，不是生成模型，并不需要表示EOF；\n",
    "\n",
    "发现这样做中间文件很不现实，数据太多，不如直接放在内存里面去训练\n",
    "\n",
    "lstm实际就是造了一个句子向量；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "载入训练数据 行 -> 1000\n",
      "载入训练数据 行 -> 2000\n",
      "载入训练数据 行 -> 3000\n",
      "载入训练数据 行 -> 4000\n",
      "载入训练数据 行 -> 5000\n",
      "载入训练数据 行 -> 6000\n",
      "载入训练数据 行 -> 7000\n",
      "载入训练数据 行 -> 8000\n",
      "载入训练数据 行 -> 9000\n",
      "载入训练数据 行 -> 10000\n",
      "载入训练数据 行 -> 11000\n"
     ]
    }
   ],
   "source": [
    "# label转tensor\n",
    "def y_tensor(y):\n",
    "    _y = torch.zeros(5)\n",
    "    _y[y] = 1\n",
    "    return _y\n",
    "\n",
    "i = 0\n",
    "# 载入训练数据\n",
    "y_x_data = []\n",
    "\n",
    "for y, s in zip(labels, sentences): # 遍历每句话\n",
    "    vectors = []\n",
    "    i += 1\n",
    "    if not i % 1000:\n",
    "        print('载入训练数据 行 -> {}'.format(i))\n",
    "\n",
    "    count = 0\n",
    "    for w in thu.cut(s): # 对分词结果进行处理\n",
    "        w = w[0]\n",
    "        if w in word_vec:\n",
    "            vec = word_vec[w]\n",
    "            count += 1\n",
    "            vectors.append(vec)\n",
    "    vectors = torch.Tensor(vectors)\n",
    "\n",
    "    # 全部0向量表示EOF\n",
    "    # sentense_vec.append(np.zeros(300))\n",
    "\n",
    "    # 至少命中3个词\n",
    "    if count >= 3:\n",
    "#         f.write(y + '\\t' + '@@'.join([json.dumps(list(w)) for w in sentense_vec]))\n",
    "        y = y_tensor(int(y))\n",
    "        y_x_data.append([y, vectors])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-1.6127, -1.6621, -1.5745, -1.5710, -1.6299]], grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-0.0237,  0.0144,  0.0101, -0.0088, -0.0007,  0.0175,  0.0379,  0.0345,\n",
       "          -0.0389,  0.0123, -0.0257, -0.0080, -0.0305, -0.0178, -0.0293, -0.0476,\n",
       "          -0.0243, -0.0090,  0.0219, -0.0192, -0.0305, -0.0271, -0.0038, -0.0323,\n",
       "           0.0291, -0.0130,  0.0411, -0.0145,  0.0430,  0.0208, -0.0319,  0.0242,\n",
       "           0.0273, -0.0152, -0.0150, -0.0295,  0.0467,  0.0187, -0.0170,  0.0183,\n",
       "           0.0384,  0.0158, -0.0310,  0.0062, -0.0129,  0.0275, -0.0174,  0.0054,\n",
       "           0.0407, -0.0123,  0.0386, -0.0369, -0.0332, -0.0350,  0.0227,  0.0442,\n",
       "          -0.0218,  0.0453, -0.0318, -0.0187,  0.0264,  0.0366, -0.0327,  0.0299,\n",
       "          -0.0110,  0.0255,  0.0438,  0.0253, -0.0205,  0.0453,  0.0004,  0.0190,\n",
       "          -0.0215,  0.0110,  0.0202, -0.0355, -0.0030, -0.0357,  0.0482,  0.0243,\n",
       "          -0.0347, -0.0423,  0.0283,  0.0108,  0.0146,  0.0362,  0.0073,  0.0251,\n",
       "           0.0057, -0.0475,  0.0357, -0.0208,  0.0022, -0.0187,  0.0002, -0.0348,\n",
       "           0.0154, -0.0394, -0.0274,  0.0080, -0.0114, -0.0432, -0.0325, -0.0451,\n",
       "          -0.0228,  0.0351, -0.0007,  0.0118,  0.0466,  0.0329,  0.0313,  0.0306,\n",
       "           0.0017,  0.0135, -0.0326, -0.0415,  0.0442,  0.0076, -0.0195, -0.0059,\n",
       "          -0.0029, -0.0382, -0.0377,  0.0237,  0.0076, -0.0438, -0.0130,  0.0262]],\n",
       "        grad_fn=<ThAddmmBackward>))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.i2o(combined)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)\n",
    "\n",
    "# 300维数据，隐藏层128维，分为5类\n",
    "rnn = RNN(300, 128, 5)\n",
    "input = torch.zeros(1, 300)\n",
    "hidden = torch.zeros(1, 128)\n",
    "rnn(input, hidden)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
