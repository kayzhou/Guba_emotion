{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 情感分类\n",
    "\n",
    "停用词\n",
    "\n",
    "加载原始文本，只考虑'1', '2', '3', '4', '-'五类，'x'不确定的暂时不考虑。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded succeed\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from collections import Counter\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import numpy as np\n",
    "# import jieba\n",
    "from thulac import thulac\n",
    "thu = thulac(seg_only=True)\n",
    "\n",
    "def load_stopword():\n",
    "    \"\"\"\n",
    "    加载停用词集合\n",
    "    \"\"\"\n",
    "    return set(json.load(open('data/stopword-zh.json')))\n",
    "\n",
    "stop_word = load_stopword()\n",
    "\n",
    "def load_label_sentence():\n",
    "    \"\"\"\n",
    "    加载原始文本\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    in_dir = 'data/labelled'\n",
    "\n",
    "    for in_name in os.listdir(in_dir):\n",
    "        _in = os.path.join(in_dir, in_name)\n",
    "        # print(_in)\n",
    "        for i, line in enumerate(open(_in)):\n",
    "            if line.strip() == '':\n",
    "                continue\n",
    "            label = line.split('\\t')[0]\n",
    "            s= line.split('\\t')[1]\n",
    "            # 1234：四种情绪，-：没有情绪，x：不确定\n",
    "            if label in ['1', '2', '3', '4', '-']:\n",
    "                if label == '-' or label == 'x':\n",
    "                    labels.append('0')\n",
    "                else:\n",
    "                    labels.append(label)\n",
    "            sentences.append(s)\n",
    "\n",
    "    return labels, sentences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "信息增益来计算特征词\n",
    "\n",
    "one-hot表示法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "读取词向量出错：行 1012\n",
      "词向量大小 2000\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n"
     ]
    }
   ],
   "source": [
    "def get_word_freq():\n",
    "    \"\"\"\n",
    "    统计高频词汇\n",
    "    \"\"\"\n",
    "    stopwords = load_stopword()\n",
    "    words_freq = {}\n",
    "    words_ci = {} # 出现某个词，是某类的概率，此问题有五类\n",
    "    class_num = 5\n",
    "    labels_num = [0] * class_num\n",
    "    labels, sentences = load_label_sentence()\n",
    "    \n",
    "    for y, s in zip(labels, sentences):\n",
    "        \n",
    "        # 统计每个类别的数量\n",
    "        labels_num[int(y)] += 1\n",
    "        # 分词\n",
    "        for w in thu.cut(s):\n",
    "            w = w[0]\n",
    "            # 停用词等过滤\n",
    "            if w == '' or w in stopwords or w.isdigit():\n",
    "                continue\n",
    "            elif w in words_freq:\n",
    "                words_freq[w] += 1\n",
    "                words_ci[w][int(y)] += 1\n",
    "            else:\n",
    "                words_freq[w] = 1\n",
    "                words_ci[w] = [0] * class_num\n",
    "                words_ci[w][int(y)] += 1\n",
    "    \n",
    "    # 数量转概率\n",
    "    num2pro = lambda nums: [num / sum(nums) for num in nums]\n",
    "    \n",
    "    # 每类上的概率\n",
    "    v_ci = num2pro(labels_num)\n",
    "    \n",
    "    word_gain = {}\n",
    "    for w in words_ci.keys():\n",
    "        word_ci = words_ci[w]\n",
    "        \n",
    "        v_ci_t = num2pro(word_ci) # 句子出现t是Ci类的概率\n",
    "        \n",
    "        non_word_ci = [labels_num[i] - word_ci[i] for i in range(class_num)] # 不是t时候的各类数量\n",
    "        v_ci_non_t = num2pro(non_word_ci) # 句子不出现t是Ci的概率\n",
    "        \n",
    "        pr_t = words_freq[w] / sum(labels_num) # 存在t的概率\n",
    "        \n",
    "        Gt = Info_gain_of_term(v_ci, v_ci_t, v_ci_non_t, pr_t)\n",
    "        \n",
    "        word_gain[w] = Gt\n",
    "        \n",
    "\n",
    "    word_gain = sorted(word_gain.items(), key=lambda d: d[1], reverse=True) \n",
    "    with open('data/word_gain_freq.txt', 'w') as f:\n",
    "        for w, gain in word_gain:\n",
    "            if words_freq[w] >= 5:\n",
    "                print(w, gain, words_freq[w], sep='\\t', file=f)\n",
    "            \n",
    "\n",
    "            \n",
    "def Info_gain_of_term(v_ci, v_ci_t, v_ci_non_t, pr_t):\n",
    "    \"\"\"\n",
    "    计算信息增益，需要每类的概率，句子出现t是Ci类的概率，不出现t是Ci的概率，存在t的概率\n",
    "    \"\"\"\n",
    "    def info_entropy(p):\n",
    "        if p == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return -p * np.log(p)\n",
    "    \n",
    "    gain = 0\n",
    "    for i in range(len(v_ci)):\n",
    "        gain = gain + (info_entropy(v_ci[i]) - pr_t * info_entropy(v_ci_t[i]) - (1 - pr_t) * info_entropy(v_ci_non_t[i]))\n",
    "    return gain\n",
    "    \n",
    "\n",
    "def word_2_vec_one_hot():\n",
    "\n",
    "    def load_word_list(first=2000):\n",
    "        word_list = []\n",
    "        for i, line in enumerate(open('data/word_gain_freq.txt')):\n",
    "            if i >= first:\n",
    "                break\n",
    "            try:\n",
    "                w, gain, freq = line.strip().split('\\t')\n",
    "            except ValueError:\n",
    "                print('读取词向量出错：行 {}'.format(i))\n",
    "            word_list.append(w)\n",
    "        print('词向量大小', len(word_list))\n",
    "        return word_list\n",
    "\n",
    "    word_list = load_word_list()\n",
    "    labels, sentences = load_label_sentence()\n",
    "    i = 0\n",
    "    for y, s in zip(labels, sentences):\n",
    "        i += 1\n",
    "        if not i % 1000:\n",
    "            print(i)\n",
    "        vec = np.zeros(len(word_list))\n",
    "        for w in thu.cut(s):\n",
    "            w = w[0]\n",
    "            # print(w)\n",
    "            try:\n",
    "                _i = word_list.index(w)\n",
    "                vec[_i] = 1\n",
    "            except ValueError:\n",
    "                pass\n",
    "\n",
    "        print(y, ','.join(['{:.1f}'.format(num) for num in list(vec)]), sep='\\t', file=open('train_data_one_hot-20180710.txt', 'a'))\n",
    "    \n",
    "# one-hot \n",
    "get_word_freq() # 词分析\n",
    "word_2_vec_one_hot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "引入ACL2018词向量（财经方面）\n",
    "\n",
    "因为该文件是按出现次数排序，那么考虑“掐头去尾”\n",
    "\n",
    "停用词要不要去？也是要考虑的，停用词有时候也起到作用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载词向量中 ...\n",
      "加载词完成！一共 149960个词\n",
      "Counter({'0': 4569, '3': 2251, '2': 2097, '1': 2095, '4': 982})\n",
      "行 -> 1000\n",
      "行 -> 2000\n",
      "行 -> 3000\n",
      "行 -> 4000\n",
      "行 -> 5000\n",
      "行 -> 6000\n",
      "行 -> 7000\n",
      "行 -> 8000\n",
      "行 -> 9000\n",
      "行 -> 10000\n",
      "行 -> 11000\n"
     ]
    }
   ],
   "source": [
    "def load_word_vec():\n",
    "    \"\"\"\n",
    "    加载ACL2018词向量，只加载信息增益筛选过后的词\n",
    "    \"\"\"\n",
    "    word_vec = {}\n",
    "    print('加载词向量中 ...')\n",
    "    for i, line in enumerate(open('data/sgns.financial.word')):\n",
    "        if i <= 10:\n",
    "            continue\n",
    "        if i > 150000:\n",
    "            break\n",
    "        words = line.strip().split(' ')\n",
    "        word = words[0]\n",
    "        word_vec[word] = np.array([float(num) for num in words[1:]])\n",
    "#         except UnicodeDecodeError:\n",
    "#             print(\"编码问题，行 {}\".format(i))\n",
    "    print('加载词完成！一共 {}个词'.format(len(word_vec)))\n",
    "    return word_vec\n",
    "\n",
    "\n",
    "word_vec = load_word_vec()\n",
    "labels, sentences = load_label_sentence()\n",
    "\n",
    "cnt = Counter(labels)\n",
    "print(cnt)\n",
    "\n",
    "\n",
    "i = 0\n",
    "# 建立训练文件\n",
    "with open('train_data_ACL-20180712.txt', 'w') as f:\n",
    "    for y, s in zip(labels, sentences):\n",
    "        i += 1\n",
    "        if not i % 1000:\n",
    "            print('行 -> {}'.format(i))\n",
    "        count = 0\n",
    "        vec = np.zeros(300)\n",
    "        \n",
    "        for w in thu.cut(s): # 对分词结果进行处理\n",
    "            w = w[0]\n",
    "#             if w in stop_word:\n",
    "#                 continue\n",
    "            if w in word_vec:\n",
    "                vec += word_vec[w]\n",
    "                count += 1\n",
    "        \n",
    "#         for w in jieba.cut(s): # 对分词结果进行处理\n",
    "#             if w in stop_word:\n",
    "#                 continue\n",
    "#             if w in word_vec:\n",
    "#                 vec += word_vec[w]\n",
    "#                 count += 1\n",
    "\n",
    "        if count != 0:\n",
    "            vec = vec / count\n",
    "            \n",
    "        if count > 0:\n",
    "            f.write(y + '\\t' + ','.join(['{:.6f}'.format(num) for num in list(vec)]) + '\\n')\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 为lstm做准备，训练数据\n",
    "\n",
    "句子不必对齐，不是生成模型，并不需要表示EOF；\n",
    "\n",
    "发现这样做中间文件很不现实，数据太多，不如直接放在内存里面去训练\n",
    "\n",
    "lstm实际就是造了一个句子向量；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载词向量中 ...\n",
      "加载词完成！一共 149971个词\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "new(): invalid data type 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-ff4dddba7664>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;31m#         f.write(y + '\\t' + '@@'.join([json.dumps(list(w)) for w in sentense_vec]))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0my_x_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectors\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-ff4dddba7664>\u001b[0m in \u001b[0;36my_tensor\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0my_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0m_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0m_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: new(): invalid data type 'str'"
     ]
    }
   ],
   "source": [
    "def load_word_vec():\n",
    "    \"\"\"\n",
    "    加载ACL2018财经类词向量\n",
    "    \"\"\"\n",
    "    word_vec = {}\n",
    "    print('加载词向量中 ...')\n",
    "    for i, line in enumerate(open('data/sgns.financial.word')):\n",
    "        if i > 150000:\n",
    "            break\n",
    "        words = line.strip().split(' ')\n",
    "        word = words[0]\n",
    "        word_vec[word] = np.array([float(num) for num in words[1:]])\n",
    "    print('加载词完成！一共 {}个词'.format(len(word_vec)))\n",
    "    return word_vec\n",
    "\n",
    "\n",
    "# label转vector\n",
    "\n",
    "def y_tensor(y):\n",
    "    _y = torch.zeros(5)\n",
    "    _y[y] = 1\n",
    "    return _y\n",
    "\n",
    "\n",
    "# 加载词向量\n",
    "word_vec = load_word_vec()\n",
    "labels, sentences = load_label_sentence()\n",
    "\n",
    "# 统计每个类别\n",
    "# cnt = Counter(labels)\n",
    "# print(cnt)\n",
    "\n",
    "\n",
    "i = 0\n",
    "# 载入训练数据\n",
    "y_x_data = []\n",
    "\n",
    "for y, s in zip(labels, sentences): # 遍历每句话\n",
    "    vectors = []\n",
    "    i += 1\n",
    "    if not i % 2000:\n",
    "        print('载入训练数据 行 -> {}'.format(i))\n",
    "\n",
    "    count = 0\n",
    "    for w in thu.cut(s): # 对分词结果进行处理\n",
    "        w = w[0]\n",
    "        if w in word_vec:\n",
    "            vec = word_vec[w]\n",
    "            count += 1\n",
    "            vectors.append(vec)\n",
    "    vectors = torch.Tensor(vectors)\n",
    "\n",
    "    # 全部0向量表示EOF\n",
    "    # sentense_vec.append(np.zeros(300))\n",
    "\n",
    "    # 至少命中3个词\n",
    "    if count >= 3:\n",
    "#         f.write(y + '\\t' + '@@'.join([json.dumps(list(w)) for w in sentense_vec]))\n",
    "        y = y_tensor(y)\n",
    "        y_x_data.append([y, vectors])\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ~ ⬆️准备训练数据 ⬇️开始训练\n",
    "\n",
    "机器学习算法包括：KNN、LR、随机森林、决策树、GBDT、SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier  \n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression  \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Multinomial Naive Bayes Classifier  \n",
    "def naive_bayes_classifier(train_x, train_y):  \n",
    "    from sklearn.naive_bayes import MultinomialNB  \n",
    "    model = MultinomialNB(alpha=0.01)  \n",
    "    model.fit(train_x, train_y)  \n",
    "    return model  \n",
    "  \n",
    "  \n",
    "# KNN Classifier  \n",
    "def knn_classifier(train_x, train_y):  \n",
    "    from sklearn.neighbors import KNeighborsClassifier  \n",
    "    model = KNeighborsClassifier()  \n",
    "    model.fit(train_x, train_y)  \n",
    "    return model  \n",
    "  \n",
    "  \n",
    "# Logistic Regression Classifier  \n",
    "def logistic_regression_classifier(train_x, train_y):  \n",
    "    from sklearn.linear_model import LogisticRegression  \n",
    "    model = LogisticRegression(penalty='l2')  \n",
    "    model.fit(train_x, train_y)  \n",
    "    return model  \n",
    "  \n",
    "  \n",
    "# Random Forest Classifier  \n",
    "def random_forest_classifier(train_x, train_y):  \n",
    "    from sklearn.ensemble import RandomForestClassifier  \n",
    "    model = RandomForestClassifier(n_estimators=8)  \n",
    "    model.fit(train_x, train_y)  \n",
    "    return model  \n",
    "  \n",
    "  \n",
    "# Decision Tree Classifier  \n",
    "def decision_tree_classifier(train_x, train_y):  \n",
    "    from sklearn import tree  \n",
    "    model = tree.DecisionTreeClassifier()  \n",
    "    model.fit(train_x, train_y)  \n",
    "    return model  \n",
    "  \n",
    "  \n",
    "# GBDT(Gradient Boosting Decision Tree) Classifier  \n",
    "def gradient_boosting_classifier(train_x, train_y):  \n",
    "    from sklearn.ensemble import GradientBoostingClassifier  \n",
    "    model = GradientBoostingClassifier(n_estimators=200)  \n",
    "    model.fit(train_x, train_y)  \n",
    "    return model  \n",
    "  \n",
    "  \n",
    "# SVM Classifier  \n",
    "def svm_classifier(train_x, train_y):  \n",
    "    from sklearn.svm import SVC  \n",
    "    model = SVC(kernel='rbf', probability=True)  \n",
    "    model.fit(train_x, train_y)  \n",
    "    return model  \n",
    "  \n",
    "# SVM Classifier using cross validation  \n",
    "def svm_cross_validation(train_x, train_y):  \n",
    "    from sklearn.grid_search import GridSearchCV  \n",
    "    from sklearn.svm import SVC  \n",
    "    model = SVC(kernel='rbf', probability=True)  \n",
    "    param_grid = {'C': [1e-3, 1e-2, 1e-1, 1, 10, 100, 1000], 'gamma': [0.001, 0.0001]}  \n",
    "    grid_search = GridSearchCV(model, param_grid, n_jobs = 1, verbose=1)  \n",
    "    grid_search.fit(train_x, train_y)  \n",
    "    best_parameters = grid_search.best_estimator_.get_params()  \n",
    "    for para, val in list(best_parameters.items()):  \n",
    "        print(para, val)  \n",
    "    model = SVC(kernel='rbf', C=best_parameters['C'], gamma=best_parameters['gamma'], probability=True)  \n",
    "    model.fit(train_x, train_y)  \n",
    "\n",
    "    \n",
    "def load_train_data(in_name):\n",
    "    \"\"\"\n",
    "    加载训练数据\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "    for line in open(in_name):\n",
    "        label, vec = line.strip().split('\\t')\n",
    "        # 高兴\n",
    "        if label == '2':\n",
    "            label = '1'\n",
    "        # 没有情绪\n",
    "        elif label == '0':\n",
    "            label = '0'\n",
    "        # 负面\n",
    "        else:\n",
    "            label = '-1'\n",
    "        x = np.array([float(v) for v in vec.split(',')])\n",
    "        y.append(label)\n",
    "        X.append(x)\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def train():\n",
    "    \"\"\"\n",
    "    调参\n",
    "    \"\"\"\n",
    "    X, y = load_train_data('data/train/train_data_ACL-20180712.txt')\n",
    "    # X, y = load_train_data('data/train/train_data_one_hot-20180710.txt')\n",
    "    print(X.shape, y.shape)\n",
    "\n",
    "    # 划分数据集\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=41)\n",
    "\n",
    "        \n",
    "    # 初始化分类器\n",
    "    test_classifiers = ['KNN', 'LR', 'RF', 'DT', 'GBDT']  \n",
    "    classifiers = {'NB':naive_bayes_classifier,   \n",
    "                  'KNN':knn_classifier,  \n",
    "                   'LR':logistic_regression_classifier,  \n",
    "                   'RF':random_forest_classifier,  \n",
    "                   'DT':decision_tree_classifier,  \n",
    "                  'SVM':svm_classifier,  \n",
    "                'SVMCV':svm_cross_validation,  \n",
    "                 'GBDT':gradient_boosting_classifier  \n",
    "    } \n",
    "    \n",
    "    for classifier in test_classifiers:  \n",
    "        print('******************* {} ********************'.format(classifier))\n",
    "        clf = classifiers[classifier](X_train, y_train)\n",
    "\n",
    "        # CV\n",
    "        print('accuracy of CV:', cross_val_score(clf, X, y, cv=5).mean())\n",
    "\n",
    "#         # 执行训练\n",
    "#         clf.fit(X_train, y_train)\n",
    "\n",
    "        # 模型评估\n",
    "        y_pred = []\n",
    "        for i in range(len(X_test)):\n",
    "            y_hat = clf.predict(X_test[i].reshape(1, -1))\n",
    "            y_pred.append(y_hat[0])\n",
    "        print(classification_report(y_test, y_pred))\n",
    "\n",
    "\n",
    "def train_model():\n",
    "    X, y = load_train_data('data/train/train_data_ACL-20180712.txt')\n",
    "    clf = LogisticRegression(penalty='l2')\n",
    "    print(X.shape, y.shape)\n",
    "    clf.fit(X, y)\n",
    "    # 保存模型\n",
    "    joblib.dump(clf, \"emo-LR-v1.model\")\n",
    "\n",
    "train()\n",
    "# train_model()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-1.6137, -1.6406, -1.6374, -1.5883, -1.5691]]), tensor(1.00000e-02 *\n",
       "        [[ 1.4097, -3.0256, -2.9220, -0.5518, -0.1959,  3.6731,  3.6946,\n",
       "          -3.5343,  1.3549, -2.3483,  1.7515,  2.4141, -2.0840, -0.6620,\n",
       "           1.5845, -3.8228,  1.7434, -2.7712, -0.8102, -3.9043, -2.8398,\n",
       "           2.0431, -3.1709, -1.9976,  3.2621, -3.1986,  3.9711,  2.3542,\n",
       "          -1.6756, -2.2272, -2.4557,  2.1466, -0.8389,  3.3536,  0.1236,\n",
       "           2.2799,  0.4585,  0.2595,  2.3904,  4.7886,  4.7961,  3.0518,\n",
       "           3.0556,  3.5622,  0.5368,  3.9230,  3.0073,  0.0646,  2.1039,\n",
       "           3.4131, -3.5795,  1.5913,  2.4635,  2.1701, -2.9343,  0.6185,\n",
       "          -0.2120,  3.2147,  3.7653,  0.0145,  3.2622,  2.2337,  4.5574,\n",
       "           2.4038, -3.7791,  1.4997,  0.7830, -3.7624, -3.2223,  0.6116,\n",
       "           0.5313, -2.3408, -0.6490, -3.7837, -4.8129,  3.2175,  4.7057,\n",
       "           0.4903, -4.7687, -3.7609, -2.9001, -2.4763,  3.3458,  4.2197,\n",
       "          -2.2795,  3.3015,  2.6840, -2.7209,  2.4093, -2.6156, -1.1948,\n",
       "           1.1006,  2.5670,  3.8362,  4.1409,  2.1993,  0.2920, -2.1982,\n",
       "          -4.4243,  0.6102,  3.6455, -1.4883,  0.7365, -4.3494, -1.4638,\n",
       "          -4.2728,  3.8654, -1.4341, -4.8203, -3.7245, -4.3859, -4.8092,\n",
       "          -3.3641, -4.2959,  2.6001, -3.2488, -0.2941,  1.3376,  0.2999,\n",
       "           0.9773,  1.0139, -0.7022, -0.9050, -1.3840, -2.5382, -3.4077,\n",
       "          -2.0178, -1.3022]]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.i2o(combined)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)\n",
    "\n",
    "# 300维数据，隐藏层128维，分为5类\n",
    "rnn = RNN(300, 128, 5)\n",
    "input = torch.zeros(1, 300)\n",
    "hidden = torch.zeros(1, 128)\n",
    "rnn(input, hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()\n",
    "\n",
    "learning_rate = 0.005\n",
    "\n",
    "def train():\n",
    "    hidden = rnn.initHidden()\n",
    "\n",
    "    rnn.zero_grad()\n",
    "\n",
    "    for i in range(len(y_x_data)):\n",
    "        output, hidden = rnn(y_x_data[i][0], hidden)\n",
    "\n",
    "    loss = criterion(output, category_tensor)\n",
    "    loss.backward()\n",
    "\n",
    "    # Add parameters' gradients to their values, multiplied by learning rate\n",
    "    for p in rnn.parameters():\n",
    "        p.data.add_(-learning_rate, p.grad.data)\n",
    "\n",
    "    return output, loss.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-9d873b1d00f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m'%dm %ds'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0miter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iters\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'time' is not defined"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "n_iters = 10000\n",
    "print_every = 5000\n",
    "plot_every = 1000\n",
    "\n",
    "\n",
    "# Keep track of losses for plotting\n",
    "current_loss = 0\n",
    "all_losses = []\n",
    "\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for iter in range(1, n_iters + 1):\n",
    "#     category, line, category_tensor, line_tensor = randomTrainingExample()\n",
    "    output, loss = train(category_tensor, line_tensor)\n",
    "    current_loss += loss\n",
    "\n",
    "    # Print iter number, loss, name and guess\n",
    "    if iter % print_every == 0:\n",
    "        guess, guess_i = categoryFromOutput(output)\n",
    "        correct = '✓' if guess == category else '✗ (%s)' % category\n",
    "        print('%d %d%% (%s) %.4f %s / %s %s' % (iter, iter / n_iters * 100, timeSince(start), loss, line, guess, correct))\n",
    "\n",
    "    # Add current loss avg to list of losses\n",
    "    if iter % plot_every == 0:\n",
    "        all_losses.append(current_loss / plot_every)\n",
    "        current_loss = 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
