{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "import os\n",
    "import string\n",
    "from pathlib import Path\n",
    "from zhon import hanzi\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import autograd, optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.metrics import classification_report\n",
    "# from tensorboardX import SummaryWriter\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from visdom import Visdom\n",
    "\n",
    "from tweet_process import TwPro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1c5e9425ca647108b0af21de98bdbc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 计算字的信息熵\n",
    "\n",
    "def Info_gain_of_term(v_ci, v_ci_t, v_ci_non_t, pr_t):\n",
    "    \"\"\"\n",
    "    计算信息增益，需要每类的概率，句子出现t是Ci类的概率，不出现t是Ci的概率，存在t的概率\n",
    "    \"\"\"\n",
    "    def info_entropy(p):\n",
    "        if p <= 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return -p * np.log(p)\n",
    "\n",
    "    gain = 0\n",
    "    for i in range(len(v_ci)):\n",
    "        gain = gain + \\\n",
    "            (info_entropy(v_ci[i]) - pr_t * info_entropy(v_ci_t[i]) - (1 - pr_t) * info_entropy(v_ci_non_t[i]))\n",
    "    return gain\n",
    "\n",
    "\n",
    "def get_word_freq():\n",
    "    \"\"\"\n",
    "    统计高频字\n",
    "    \"\"\"\n",
    "    words_freq = {}\n",
    "    words_ci = {} # 出现某个词，是某类的概率，此问题有五类\n",
    "    class_num = 4\n",
    "    labels_num = np.zeros(class_num)\n",
    "\n",
    "    for line in tqdm(open(config.train_file)):\n",
    "        label, sentence = line.strip().split(\"\\t\")\n",
    "        label = label.strip()\n",
    "        if label == \"-\":\n",
    "            label = 0\n",
    "        elif label == \"x\":\n",
    "            continue\n",
    "        else:\n",
    "            label = int(label)\n",
    "                \n",
    "        # 四分类\n",
    "        if class_num == 4:\n",
    "            if label == 0:\n",
    "                continue\n",
    "            label -= 1\n",
    "            \n",
    "            labels_num[label] += 1\n",
    "            \n",
    "        sentence = sentence.replace(\" \", \"\")\n",
    "        \n",
    "        for char in sentence:\n",
    "            if char not in string.punctuation and char not in string.digits and \\\n",
    "                char not in hanzi.punctuation and char not in string.ascii_letters:\n",
    "                \n",
    "                if char in words_freq:\n",
    "                    words_freq[char] += 1\n",
    "                    words_ci[char][label] += 1\n",
    "                else:\n",
    "                    words_freq[char] = 1\n",
    "                    words_ci[char] = np.zeros(class_num)\n",
    "                    words_ci[char][label] += 1\n",
    "                    \n",
    "\n",
    "    # 数量转概率\n",
    "    num2pro = lambda nums: [num / sum(nums) for num in nums]\n",
    "\n",
    "    # 每类上的概率\n",
    "    v_ci = num2pro(labels_num)\n",
    "\n",
    "    word_gain = {}\n",
    "    for w in words_ci.keys():\n",
    "        word_ci = words_ci[w]\n",
    "        v_ci_t = num2pro(word_ci) # 句子出现t是Ci类的概率\n",
    "        non_word_ci = [labels_num[i] - word_ci[i] for i in range(class_num)] # 不是t时候的各类数量\n",
    "        v_ci_non_t = num2pro(non_word_ci) # 句子不出现t是Ci的概率\n",
    "        pr_t = words_freq[w] / sum(labels_num) # 存在t的概率\n",
    "        Gt = Info_gain_of_term(v_ci, v_ci_t, v_ci_non_t, pr_t)\n",
    "        word_gain[w] = Gt\n",
    "\n",
    "\n",
    "    word_gain = sorted(word_gain.items(), key=lambda d: d[1], reverse=True) # 根据信息增益排序\n",
    "    with open('data/char_gain_freq.txt', 'w') as f:\n",
    "        for w, gain in word_gain:\n",
    "            if words_freq[w] >= 10:\n",
    "                print(w, gain, words_freq[w], sep='\\t', file=f)\n",
    "                \n",
    "get_word_freq()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.train_file = \"data/2019-02-12/train.txt\"\n",
    "        self.batch_size = 128\n",
    "        self.embedding_size = 64 # word embedding\n",
    "        self.channel = 3\n",
    "        self.learning_rate = 0.001\n",
    "        self.window_size = 3\n",
    "        self.num_classes = 4\n",
    "\n",
    "        self.num_epochs = 20\n",
    "        self.summary_interval = 20\n",
    "        \n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "class MYDataset(Dataset):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._wv1 = None\n",
    "        self._wv2 = None\n",
    "        self._wv3 = self.read_wv3()\n",
    "        \n",
    "        if not Path(\"data/y.npy\").exists():\n",
    "            self._calc()\n",
    "        else:\n",
    "            self._load()\n",
    "            return\n",
    "\n",
    "        # 根据分类目标，调整X和y\n",
    "        num_c = config.num_classes\n",
    "        \n",
    "        if num_c == 2:\n",
    "            print(num_c, \"...\")\n",
    "            tmp_X = []\n",
    "            tmp_y = []\n",
    "            for x, y in zip(self.X, self.y):\n",
    "                if y == 0:\n",
    "                    tmp_X.append(x)\n",
    "                    tmp_y.append(0)\n",
    "                else:\n",
    "                    tmp_X.append(x)\n",
    "                    tmp_y.append(1)\n",
    "            self.X = np.array(tmp_X)\n",
    "            self.y = np.array(tmp_y)\n",
    "            \n",
    "        elif num_c == 4:\n",
    "            print(num_c, \"...\")\n",
    "            tmp_X = []\n",
    "            tmp_y = []\n",
    "            for x, y in zip(self.X, self.y):\n",
    "                if y == 0:\n",
    "                    continue\n",
    "                else:\n",
    "                    tmp_X.append(x)\n",
    "                    tmp_y.append(y - 1)\n",
    "            self.X = np.array(tmp_X)\n",
    "            self.y = np.array(tmp_y)\n",
    "            \n",
    "        self._save()\n",
    "                    \n",
    "        \n",
    "        # 选择是否采样\n",
    "        # self.sample()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def _weight(self):\n",
    "        return Counter(self.y)\n",
    "        \n",
    "    def read_wv1(self):\n",
    "        print(\"Loading wv1 ...\")\n",
    "        return Word2Vec.load(\"model/guba_word2vec.model\")\n",
    "\n",
    "    def read_wv2(self):\n",
    "        \"\"\"\n",
    "        加载ACL2018词向量\n",
    "        \"\"\"\n",
    "        word_vec = {}\n",
    "        print('Loading wv2 ...')\n",
    "        for i, line in enumerate(open('model/sgns.financial.word')):\n",
    "            if i > 200000:\n",
    "                break\n",
    "            words = line.strip().split(' ')\n",
    "            word = words[0]\n",
    "            word_vec[word] = np.array([float(num) for num in words[1:]])\n",
    "    #         except UnicodeDecodeError:\n",
    "    #             print(\"编码问题，行 {}\".format(i))\n",
    "        print('Loaded! There are {} words.'.format(len(word_vec)))\n",
    "        return word_vec\n",
    "\n",
    "    def read_wv3(self):\n",
    "        char_list = []\n",
    "        for line in open(\"data/char_gain_freq.txt\"):\n",
    "            char = line.strip().split(\"\\t\")[0]\n",
    "            if char not in string.punctuation and char not in string.digits and \\\n",
    "                char not in hanzi.punctuation and char not in string.ascii_letters:\n",
    "                char_list.append(char)\n",
    "\n",
    "        char_list = char_list[: 300]\n",
    "        # print(char_list)\n",
    "        count_v = CountVectorizer(vocabulary=char_list, analyzer=\"char\", binary=True)\n",
    "        count_v.fit([\"hello world\"])\n",
    "        return count_v\n",
    "\n",
    "    def wv1(self, words):\n",
    "        v = np.zeros(config.embedding_size * 300).reshape(config.embedding_size, 300)\n",
    "        _index = 0\n",
    "        for w in words:\n",
    "            if _index >= config.embedding_size:\n",
    "                break\n",
    "            if w in self._wv1.wv:\n",
    "                v[_index] = self._wv1.wv[w]\n",
    "                _index += 1\n",
    "        return v\n",
    "\n",
    "    def wv2(self, words):\n",
    "        v = np.zeros(config.embedding_size * 300).reshape(config.embedding_size, 300)\n",
    "        _index = 0\n",
    "        for w in words:\n",
    "            if _index >= config.embedding_size:\n",
    "                break\n",
    "            if w in self._wv2:\n",
    "                v[_index] = self._wv2[w]\n",
    "                _index += 1\n",
    "        return v\n",
    "    \n",
    "    def wv3(self, text):\n",
    "        v = np.zeros(config.embedding_size * 300).reshape(config.embedding_size, 300)\n",
    "        _index = 0\n",
    "        for char in text:\n",
    "            if _index >= config.embedding_size:\n",
    "                break\n",
    "            if char in self._wv3.vocabulary_:\n",
    "                pos = self._wv3.vocabulary_.get(char)\n",
    "                v[_index][pos] = 1 \n",
    "                _index += 1\n",
    "        return v\n",
    "    \n",
    "    def sample(self):\n",
    "        X, y = self.X, self.y\n",
    "        X = X.reshape(-1, 2 * 100 * 300)\n",
    "        ros = RandomOverSampler(random_state=13)\n",
    "        X_resampled, y_resampled = ros.fit_sample(X, y)\n",
    "        X_resampled = X.reshape(-1, 2, 100, 300)\n",
    "        self.X, self.y = X_resampled, y_resampled\n",
    "        \n",
    "    def _calc(self):\n",
    "        if not self._wv1:\n",
    "            self._wv1 = self.read_wv1()\n",
    "        if not self._wv2:\n",
    "            self._wv2 = self.read_wv2()\n",
    "\n",
    "        X = []\n",
    "        y = []\n",
    "        for line in tqdm(open(config.train_file)):\n",
    "            try:\n",
    "                label, sentence = line.strip().split(\"\\t\")\n",
    "            except ValueError:\n",
    "                print(\"Error line:\", line)\n",
    "                continue\n",
    "\n",
    "            # 5- 分类\n",
    "            label = label.strip()\n",
    "            if label == \"-\":\n",
    "                label = 0\n",
    "            elif label == \"x\":\n",
    "                continue\n",
    "            else:\n",
    "                label = int(label)\n",
    "            y.append(label)\n",
    "            \n",
    "            words = sentence.split()\n",
    "            \n",
    "            if config.channel == 2:\n",
    "                X.append(np.array([self.wv1(words), self.wv2(words)]))\n",
    "            elif config.channel == 3:\n",
    "                X.append(np.array([self.wv1(words), self.wv2(words), self.wv3(sentence)]))\n",
    "                \n",
    "        self.X, self.y = X, y\n",
    "    \n",
    "    def _save(self):\n",
    "        print(\"Saving ...\")\n",
    "        np.save(\"data/X.npy\", np.array(self.X))\n",
    "        np.save(\"data/y.npy\", np.array(self.y))\n",
    "    \n",
    "    def _load(self):\n",
    "        # self.clear()\n",
    "        if Path(\"data/y.npy\").exists():\n",
    "            print(\"Loading X and y...\")\n",
    "            self.X = np.load(\"data/X.npy\")\n",
    "            self.y = np.load(\"data/X.npy\")\n",
    "        else:\n",
    "            print(\"数据文件不存在。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading wv1 ...\n",
      "Loading wv2 ...\n",
      "Loaded! There are 199948 words.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00cafb84cd5c4b1eae8ca38565003dce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4 ...\n",
      "Saving ...\n",
      "CPU times: user 44.1 s, sys: 32.1 s, total: 1min 16s\n",
      "Wall time: 1min 29s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9550"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time my_data = MYDataset()\n",
    "len(my_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({2: 2703, 1: 2614, 0: 2473, 3: 1760})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.0004, 0.0004, 0.0004, 0.0006])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rst = my_data._weight()\n",
    "print(rst)\n",
    "sample_weight = np.array([1 / rst[i] for i in range(config.num_classes)])\n",
    "sample_weight = torch.from_numpy(sample_weight).float()\n",
    "# sampler = torch.utils.data.sampler.WeightedRandomSampler(sample_weight, num_samples=len(my_data))\n",
    "sample_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 50, 300)\n"
     ]
    }
   ],
   "source": [
    "test_split = 0.1\n",
    "\n",
    "# dataset_size = len(my_data)\n",
    "# indices = list(range(dataset_size))\n",
    "# split = int(np.floor(validation_split * dataset_size))\n",
    "# np.random.seed(13)\n",
    "# np.random.shuffle(indices)\n",
    "# train_indices, val_indices = indices[:split], indices[split:]\n",
    "# train_data = my_data[train_indices]\n",
    "# val_data = my_data[val_indices]\n",
    "\n",
    "# test_size = int(test_split * len(my_data))\n",
    "# train_size = len(my_data) - test_size\n",
    "# train_data, test_data = torch.utils.data.random_split(my_data, [train_size, test_size])\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_data, test_data = train_test_split(my_data, test_size=test_split, random_state=21)\n",
    "print(train_data[0][0].shape)\n",
    "# train_loader = torch.utils.data.DataLoader(train_data, batch_size=config.batch_size, \n",
    "#                                            sampler=sampler)\n",
    "# test_loader = torch.utils.data.DataLoader(test_data, batch_size=1000,\n",
    "#                                                 sampler=sampler)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=config.batch_size)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CNNClassifier, self).__init__()\n",
    "        # 2 in- channels, 32 out- channels, 3 * 300 windows size\n",
    "        self.conv = torch.nn.Conv2d(config.channel, 32 * config.channel, kernel_size=(3, 300), groups=config.channel)\n",
    "        \n",
    "        self.f1 = nn.Linear(16 * config.channel * (config.embedding_size - 2), 256)\n",
    "        self.f2 = nn.Linear(256, 128)\n",
    "        self.f3 = nn.Linear(128, 64)\n",
    "        self.f4 = nn.Linear(64, 32)\n",
    "        self.f5 = nn.Linear(32, config.num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "#         print(x.size())\n",
    "        x = x.float()\n",
    "        out = F.relu(self.conv(x))\n",
    "#         print(out.size())\n",
    "        out = torch.squeeze(out)\n",
    "#         print(out.size())\n",
    "        out = F.max_pool1d(out, 2)\n",
    "#         print(out.size())\n",
    "        out = out.view(-1, 16 * config.channel * (config.embedding_size - 2))\n",
    "#         print(out.size())\n",
    "        out = F.relu(out)\n",
    "    \n",
    "    \n",
    "        out = F.relu(self.f1(out))\n",
    "#         print(out.size())\n",
    "        out = F.relu(self.f2(out))\n",
    "        out = F.relu(self.f3(out))\n",
    "        out = F.relu(self.f4(out))\n",
    "        out = F.relu(self.f5(out))\n",
    "        \n",
    "        # print(out.size())\n",
    "        \n",
    "        if config.num_classes == 2:\n",
    "            probs = F.log_softmax(out, dim=1)\n",
    "        else:\n",
    "            probs = out\n",
    "            \n",
    "        # print(probs, probs.size())\n",
    "        # print(probs.size())\n",
    "        classes = torch.max(probs, -1)[1]\n",
    "\n",
    "        return probs, classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model):\n",
    "#     vis = Visdom()\n",
    "#     lay1 = dict(title=\"train loss\", xaxis={'title': 'step'}, yaxis={'title': 'loss'})\n",
    "#     win1 = vis.line(X = np.array([0]), Y = np.array([0]), opts=lay1)\n",
    "#     lay2 = dict(title=\"avg precision\", xaxis={'title': 'epoch'}, yaxis={'title': 'precision'})\n",
    "#     win2 = vis.line(X = np.array([0]), Y = np.array([0]), opts=lay2)\n",
    "    \n",
    "    step_loss = []\n",
    "    step_precision = []\n",
    "\n",
    "    num_c = config.num_classes\n",
    "    if num_c == 2:\n",
    "        loss_function = nn.NLLLoss(sample_weight)\n",
    "    else:\n",
    "        # loss_function = nn.CrossEntropyLoss(sample_weight)\n",
    "        loss_function = nn.CrossEntropyLoss(sample_weight)\n",
    "        \n",
    "    # optimizer = optim.SGD(model.parameters(), lr=config.learning_rate)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "\n",
    "    # writer = SummaryWriter(log_dir=\"log\")\n",
    "\n",
    "    epoch = 0\n",
    "    step = 0\n",
    "\n",
    "    for epoch in range(1, config.num_epochs + 1):\n",
    "        print(\"============== Epoch: {} ==============\".format(epoch))\n",
    "        running_losses = []\n",
    "\n",
    "        for sequences, labels in train_loader:\n",
    "            # Predict\n",
    "            # model.zero_grad()\n",
    "            probs, classes = model(sequences)\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            losses = loss_function(probs, labels)\n",
    "            losses.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Log summary\n",
    "            running_losses.append(losses.data.item())\n",
    "            if step % config.summary_interval == 0:\n",
    "                loss = sum(running_losses) / len(running_losses)\n",
    "                # writer.add_scalar(\"train/loss\", loss, step)\n",
    "                step_loss.append([step, loss])\n",
    "                \n",
    "#                 vis.line(X=np.array([step]), Y=np.array([loss]), update='append', win=win1)\n",
    "                print(\"step = {}, loss = {:.4f}\".format(step, loss))\n",
    "            if loss < 0.1:\n",
    "            \n",
    "                running_losses = []\n",
    "            step += 1\n",
    "\n",
    "        # Classification report\n",
    "        for vali in test_loader:\n",
    "            X_test = vali[0]\n",
    "            y_test = vali[1]\n",
    "            probs, y_pred = model(X_test)\n",
    "            # print(y_pred)\n",
    "            if num_c == 5:\n",
    "                target_names = ['non-', 'anger', \"joy\", \"sadness\", \"fear\"]\n",
    "            elif num_c == 4:\n",
    "                target_names = ['anger', \"joy\", \"sadness\", \"fear\"]\n",
    "            elif num_c == 2:\n",
    "                target_names = ['non-', \"emotinal\"]\n",
    "            # logging.info(\"{}\".format(classification_report(y_test, y_pred, target_names=target_names)))\n",
    "            # report = classification_report(y_test, y_pred, target_names=target_names, output_dict=True)\n",
    "            print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "            \n",
    "#             vis.line(X=np.array([epoch]), Y=np.array([report[\"micro avg\"][\"precision\"]]), update='append', win=win2)\n",
    "            break\n",
    "    \n",
    "        if loss < 0.1:\n",
    "            print(\"loss < 0.1\")\n",
    "            break\n",
    "\n",
    "        epoch += 1\n",
    "\n",
    "    # Save\n",
    "    # torch.save(model, \"model/shit{}.pkl\".format(epoch))\n",
    "    \n",
    "\n",
    "#     step_loss = np.array(step_loss)\n",
    "#     step_precision = np.array(step_precision)\n",
    "#     viz.line(X=step_loss[:, 0], Y=step_loss[:, 1])\n",
    "#     viz.line(X=step_precision[:, 0], Y=step_precision[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNClassifier(\n",
      "  (conv): Conv2d(3, 96, kernel_size=(3, 300), stride=(1, 1), groups=3)\n",
      "  (f1): Linear(in_features=2304, out_features=256, bias=True)\n",
      "  (f2): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (f3): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (f4): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (f5): Linear(in_features=32, out_features=4, bias=True)\n",
      ")\n",
      "============== Epoch: 1 ==============\n",
      "step = 0, loss = 1.3803\n",
      "step = 20, loss = 1.3652\n",
      "step = 40, loss = 1.3061\n",
      "step = 60, loss = 1.2359\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.48      0.68      0.57       226\n",
      "         joy       0.73      0.42      0.53       272\n",
      "     sadness       0.38      0.40      0.39       267\n",
      "        fear       0.57      0.61      0.59       190\n",
      "\n",
      "   micro avg       0.51      0.51      0.51       955\n",
      "   macro avg       0.54      0.53      0.52       955\n",
      "weighted avg       0.54      0.51      0.51       955\n",
      "\n",
      "============== Epoch: 2 ==============\n",
      "step = 80, loss = 0.9953\n",
      "step = 100, loss = 0.9823\n",
      "step = 120, loss = 0.9455\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.54      0.73      0.62       226\n",
      "         joy       0.78      0.55      0.64       272\n",
      "     sadness       0.48      0.42      0.45       267\n",
      "        fear       0.58      0.68      0.63       190\n",
      "\n",
      "   micro avg       0.58      0.58      0.58       955\n",
      "   macro avg       0.59      0.60      0.58       955\n",
      "weighted avg       0.60      0.58      0.58       955\n",
      "\n",
      "============== Epoch: 3 ==============\n",
      "step = 140, loss = 0.8649\n",
      "step = 160, loss = 0.8391\n",
      "step = 180, loss = 0.8221\n",
      "step = 200, loss = 0.7797\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.51      0.75      0.61       226\n",
      "         joy       0.73      0.64      0.68       272\n",
      "     sadness       0.52      0.31      0.39       267\n",
      "        fear       0.59      0.71      0.64       190\n",
      "\n",
      "   micro avg       0.59      0.59      0.59       955\n",
      "   macro avg       0.59      0.60      0.58       955\n",
      "weighted avg       0.59      0.59      0.57       955\n",
      "\n",
      "============== Epoch: 4 ==============\n",
      "step = 220, loss = 0.6863\n",
      "step = 240, loss = 0.6953\n",
      "step = 260, loss = 0.6461\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.58      0.62      0.60       226\n",
      "         joy       0.73      0.54      0.62       272\n",
      "     sadness       0.52      0.36      0.42       267\n",
      "        fear       0.46      0.79      0.58       190\n",
      "\n",
      "   micro avg       0.56      0.56      0.56       955\n",
      "   macro avg       0.57      0.58      0.56       955\n",
      "weighted avg       0.58      0.56      0.55       955\n",
      "\n",
      "============== Epoch: 5 ==============\n",
      "step = 280, loss = 0.4953\n",
      "step = 300, loss = 0.5404\n",
      "step = 320, loss = 0.5440\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.47      0.75      0.58       226\n",
      "         joy       0.68      0.52      0.59       272\n",
      "     sadness       0.45      0.48      0.47       267\n",
      "        fear       0.86      0.45      0.59       190\n",
      "\n",
      "   micro avg       0.55      0.55      0.55       955\n",
      "   macro avg       0.61      0.55      0.56       955\n",
      "weighted avg       0.60      0.55      0.55       955\n",
      "\n",
      "============== Epoch: 6 ==============\n",
      "step = 340, loss = 0.7793\n",
      "step = 360, loss = 0.4604\n",
      "step = 380, loss = 0.5066\n",
      "step = 400, loss = 0.4656\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.65      0.50      0.56       226\n",
      "         joy       0.58      0.78      0.67       272\n",
      "     sadness       0.50      0.57      0.53       267\n",
      "        fear       0.80      0.49      0.61       190\n",
      "\n",
      "   micro avg       0.60      0.60      0.60       955\n",
      "   macro avg       0.63      0.58      0.59       955\n",
      "weighted avg       0.62      0.60      0.59       955\n",
      "\n",
      "============== Epoch: 7 ==============\n",
      "step = 420, loss = 0.4516\n",
      "step = 440, loss = 0.4867\n",
      "step = 460, loss = 0.4297\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.67      0.50      0.57       226\n",
      "         joy       0.71      0.52      0.60       272\n",
      "     sadness       0.42      0.82      0.56       267\n",
      "        fear       0.92      0.35      0.51       190\n",
      "\n",
      "   micro avg       0.57      0.57      0.57       955\n",
      "   macro avg       0.68      0.55      0.56       955\n",
      "weighted avg       0.66      0.57      0.56       955\n",
      "\n",
      "============== Epoch: 8 ==============\n",
      "step = 480, loss = 0.3579\n",
      "step = 500, loss = 0.3536\n",
      "step = 520, loss = 0.3318\n",
      "step = 540, loss = 0.2966\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.57      0.60      0.58       226\n",
      "         joy       0.77      0.35      0.48       272\n",
      "     sadness       0.46      0.61      0.52       267\n",
      "        fear       0.56      0.68      0.61       190\n",
      "\n",
      "   micro avg       0.55      0.55      0.55       955\n",
      "   macro avg       0.59      0.56      0.55       955\n",
      "weighted avg       0.59      0.55      0.54       955\n",
      "\n",
      "============== Epoch: 9 ==============\n",
      "step = 560, loss = 0.2887\n",
      "step = 580, loss = 0.3026\n",
      "step = 600, loss = 0.2837\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.66      0.36      0.47       226\n",
      "         joy       0.56      0.75      0.64       272\n",
      "     sadness       0.51      0.54      0.53       267\n",
      "        fear       0.61      0.62      0.62       190\n",
      "\n",
      "   micro avg       0.57      0.57      0.57       955\n",
      "   macro avg       0.59      0.57      0.56       955\n",
      "weighted avg       0.58      0.57      0.56       955\n",
      "\n",
      "============== Epoch: 10 ==============\n",
      "step = 620, loss = 0.2367\n",
      "step = 640, loss = 0.2271\n",
      "step = 660, loss = 0.2268\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.67      0.41      0.51       226\n",
      "         joy       0.62      0.68      0.65       272\n",
      "     sadness       0.51      0.51      0.51       267\n",
      "        fear       0.50      0.68      0.58       190\n",
      "\n",
      "   micro avg       0.57      0.57      0.57       955\n",
      "   macro avg       0.58      0.57      0.56       955\n",
      "weighted avg       0.58      0.57      0.56       955\n",
      "\n",
      "============== Epoch: 11 ==============\n",
      "step = 680, loss = 0.1539\n",
      "step = 700, loss = 0.2139\n",
      "step = 720, loss = 0.1818\n",
      "step = 740, loss = 0.1656\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.62      0.43      0.51       226\n",
      "         joy       0.71      0.47      0.57       272\n",
      "     sadness       0.44      0.62      0.52       267\n",
      "        fear       0.53      0.67      0.59       190\n",
      "\n",
      "   micro avg       0.55      0.55      0.55       955\n",
      "   macro avg       0.58      0.55      0.55       955\n",
      "weighted avg       0.58      0.55      0.55       955\n",
      "\n",
      "============== Epoch: 12 ==============\n",
      "step = 760, loss = 0.1410\n",
      "step = 780, loss = 0.1672\n",
      "step = 800, loss = 0.1584\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.65      0.25      0.36       226\n",
      "         joy       0.69      0.50      0.58       272\n",
      "     sadness       0.39      0.81      0.52       267\n",
      "        fear       0.75      0.47      0.58       190\n",
      "\n",
      "   micro avg       0.52      0.52      0.52       955\n",
      "   macro avg       0.62      0.51      0.51       955\n",
      "weighted avg       0.61      0.52      0.51       955\n",
      "\n",
      "============== Epoch: 13 ==============\n",
      "step = 820, loss = 0.1928\n",
      "step = 840, loss = 0.1727\n",
      "step = 860, loss = 0.1954\n",
      "step = 880, loss = 0.1972\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.54      0.54      0.54       226\n",
      "         joy       0.66      0.47      0.55       272\n",
      "     sadness       0.40      0.62      0.49       267\n",
      "        fear       0.73      0.47      0.58       190\n",
      "\n",
      "   micro avg       0.53      0.53      0.53       955\n",
      "   macro avg       0.58      0.53      0.54       955\n",
      "weighted avg       0.57      0.53      0.54       955\n",
      "\n",
      "============== Epoch: 14 ==============\n",
      "step = 900, loss = 0.2006\n",
      "step = 920, loss = 0.2471\n",
      "step = 940, loss = 0.2197\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.54      0.60      0.57       226\n",
      "         joy       0.62      0.64      0.63       272\n",
      "     sadness       0.46      0.58      0.51       267\n",
      "        fear       0.79      0.37      0.50       190\n",
      "\n",
      "   micro avg       0.56      0.56      0.56       955\n",
      "   macro avg       0.60      0.55      0.55       955\n",
      "weighted avg       0.59      0.56      0.56       955\n",
      "\n",
      "============== Epoch: 15 ==============\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 960, loss = 0.1218\n",
      "step = 980, loss = 0.1633\n",
      "step = 1000, loss = 0.1894\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.53      0.67      0.59       226\n",
      "         joy       0.61      0.68      0.64       272\n",
      "     sadness       0.53      0.51      0.52       267\n",
      "        fear       0.73      0.45      0.56       190\n",
      "\n",
      "   micro avg       0.58      0.58      0.58       955\n",
      "   macro avg       0.60      0.57      0.58       955\n",
      "weighted avg       0.59      0.58      0.58       955\n",
      "\n",
      "============== Epoch: 16 ==============\n",
      "step = 1020, loss = 0.1479\n",
      "step = 1040, loss = 0.0915\n",
      "step = 1060, loss = 0.1042\n",
      "step = 1080, loss = 0.0663\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.49      0.73      0.59       226\n",
      "         joy       0.65      0.64      0.65       272\n",
      "     sadness       0.51      0.42      0.46       267\n",
      "        fear       0.72      0.49      0.59       190\n",
      "\n",
      "   micro avg       0.57      0.57      0.57       955\n",
      "   macro avg       0.59      0.57      0.57       955\n",
      "weighted avg       0.59      0.57      0.57       955\n",
      "\n",
      "loss < 0.1\n",
      "CPU times: user 14min 56s, sys: 10 s, total: 15min 6s\n",
      "Wall time: 2min 17s\n"
     ]
    }
   ],
   "source": [
    "model = CNNClassifier()\n",
    "print(model)\n",
    "%time train(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
