{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorboardX import SummaryWriter\n",
    "from torch import autograd, optim\n",
    "from tqdm import tqdm\n",
    "from visdom import Visdom\n",
    "\n",
    "from tweet_process import TwPro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.train_file = \"data/2019-02-12/train.txt\"\n",
    "        self.batch_size = 128\n",
    "        self.embedding_size = 50 # word embedding\n",
    "\n",
    "        self.learning_rate = 0.001\n",
    "        self.window_size = 3\n",
    "        # self.num_classes = 2\n",
    "\n",
    "        self.num_epochs = 10\n",
    "        self.train_steps = None\n",
    "\n",
    "        self.summary_interval = 10\n",
    "        \n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "\n",
    "num_c = 4\n",
    "\n",
    "class MYDataset(Dataset):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._wv1 = None\n",
    "        self._wv2 = None\n",
    "        self.X, self.y = self._load() \n",
    "\n",
    "        if num_c == 2:\n",
    "            print(num_c, \"...\")\n",
    "            tmp_X = []\n",
    "            tmp_y = []\n",
    "            for x, y in zip(self.X, self.y):\n",
    "                if y == 0:\n",
    "                    tmp_X.append(x)\n",
    "                    tmp_y.append(0)\n",
    "                else:\n",
    "                    tmp_X.append(x)\n",
    "                    tmp_y.append(1)\n",
    "            self.X = np.array(tmp_X)\n",
    "            self.y = np.array(tmp_y)\n",
    "            \n",
    "        elif num_c == 4:\n",
    "            print(num_c, \"...\")\n",
    "            tmp_X = []\n",
    "            tmp_y = []\n",
    "            for x, y in zip(self.X, self.y):\n",
    "                if y == 0:\n",
    "                    continue\n",
    "                else:\n",
    "                    tmp_X.append(x)\n",
    "                    tmp_y.append(y - 1)\n",
    "            self.X = np.array(tmp_X)\n",
    "            self.y = np.array(tmp_y)\n",
    "                    \n",
    "        \n",
    "        # 选择是否采样\n",
    "        # self.sample()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def _weight(self):\n",
    "        return Counter(self.y)\n",
    "        \n",
    "    def read_wv1(self):\n",
    "        print(\"Loading wv1 ...\")\n",
    "        return Word2Vec.load(\"model/guba_word2vec.model\")\n",
    "\n",
    "    def read_wv2(self):\n",
    "        \"\"\"\n",
    "        加载ACL2018词向量\n",
    "        \"\"\"\n",
    "        word_vec = {}\n",
    "        print('Loading wv2 ...')\n",
    "        for i, line in enumerate(open('model/sgns.financial.word')):\n",
    "            if i > 500000:\n",
    "                break\n",
    "            words = line.strip().split(' ')\n",
    "            word = words[0]\n",
    "            word_vec[word] = np.array([float(num) for num in words[1:]])\n",
    "    #         except UnicodeDecodeError:\n",
    "    #             print(\"编码问题，行 {}\".format(i))\n",
    "        print('Loaded! There are {} words.'.format(len(word_vec)))\n",
    "        return word_vec\n",
    "\n",
    "    def wv1(self, words):\n",
    "        v = np.zeros(config.embedding_size * 300).reshape(config.embedding_size, 300)\n",
    "        _index = 0\n",
    "        for w in words:\n",
    "            if _index >= config.embedding_size:\n",
    "                break\n",
    "            if w in self._wv1.wv:\n",
    "                v[_index] = self._wv1.wv[w]\n",
    "                _index += 1\n",
    "        return v\n",
    "\n",
    "    def wv2(self, words):\n",
    "        v = np.zeros(config.embedding_size * 300).reshape(config.embedding_size, 300)\n",
    "        _index = 0\n",
    "        for w in words:\n",
    "            if _index >= config.embedding_size:\n",
    "                break\n",
    "            if w in self._wv2:\n",
    "                v[_index] = self._wv2[w]\n",
    "                _index += 1\n",
    "        return v\n",
    "\n",
    "    \n",
    "    def sample(self):\n",
    "        X, y = self.X, self.y\n",
    "        X = X.reshape(-1, 2 * 100 * 300)\n",
    "        ros = RandomOverSampler(random_state=13)\n",
    "        X_resampled, y_resampled = ros.fit_sample(X, y)\n",
    "        X_resampled = X.reshape(-1, 2, 100, 300)\n",
    "        self.X, self.y = X_resampled, y_resampled\n",
    "        \n",
    "    def _save(self):\n",
    "        \n",
    "        if not self._wv1:\n",
    "            self._wv1 = self.read_wv1()\n",
    "        if not self._wv2:\n",
    "            self._wv2 = self.read_wv2()\n",
    "\n",
    "        X = []; y = []\n",
    "        for line in open(config.train_file):\n",
    "            try:\n",
    "                label, sentence = line.strip().split(\"\\t\")\n",
    "            except ValueError:\n",
    "                print(\"Error line:\", line)\n",
    "                continue\n",
    "\n",
    "            # 5- 分类\n",
    "            label = label.strip()\n",
    "            if label == \"-\":\n",
    "                label = 0\n",
    "            elif label == \"x\":\n",
    "                continue\n",
    "            else:\n",
    "                label = int(label)\n",
    "            y.append(label)\n",
    "            \n",
    "            words = sentence.split()\n",
    "            X.append(np.array([self.wv1(words), self.wv2(words)]))\n",
    "            \n",
    "        np.save(\"data/X.npy\", np.array(X))\n",
    "        np.save(\"data/y.npy\", np.array(y))\n",
    "        \n",
    "    def _load(self):\n",
    "        if not Path(\"data/X.npy\").exists():\n",
    "            self._save()\n",
    "        X = np.load(\"data/X.npy\")\n",
    "        y = np.load(\"data/y.npy\")\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading wv1 ...\n",
      "Loading wv2 ...\n",
      "Loaded! There are 467266 words.\n",
      "4 ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9603"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_data = MYDataset()\n",
    "len(my_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0004, 0.0004, 0.0004, 0.0006])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rst = my_data._weight()\n",
    "sample_weight = np.array([1 / rst[i] for i in range(num_c)])\n",
    "sample_weight = torch.from_numpy(sample_weight).float()\n",
    "# sampler = torch.utils.data.sampler.WeightedRandomSampler(sample_weight, num_samples=len(my_data))\n",
    "sample_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_split = 0.9\n",
    "\n",
    "# dataset_size = len(my_data)\n",
    "# indices = list(range(dataset_size))\n",
    "# split = int(np.floor(validation_split * dataset_size))\n",
    "# np.random.seed(13)\n",
    "# np.random.shuffle(indices)\n",
    "# train_indices, val_indices = indices[:split], indices[split:]\n",
    "# train_data = my_data[train_indices]\n",
    "# val_data = my_data[val_indices]\n",
    "\n",
    "train_size = int(test_split * len(my_data))\n",
    "test_size = len(my_data) - train_size\n",
    "train_data, test_data = torch.utils.data.random_split(my_data, [train_size, test_size])\n",
    "\n",
    "# train_loader = torch.utils.data.DataLoader(train_data, batch_size=config.batch_size, \n",
    "#                                            sampler=sampler)\n",
    "# test_loader = torch.utils.data.DataLoader(test_data, batch_size=1000,\n",
    "#                                                 sampler=sampler)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=config.batch_size)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CNNClassifier, self).__init__()\n",
    "        # 2 in- channels, 32 out- channels, 3 * 300 windows size\n",
    "        self.conv = torch.nn.Conv2d(2, 64, kernel_size=(3, 300), groups=2)\n",
    "        \n",
    "        self.f1 = nn.Linear(32 * (config.embedding_size - 2), 128)\n",
    "        self.f2 = nn.Linear(256, 128)\n",
    "        self.f3 = nn.Linear(128, 64)\n",
    "        self.f4 = nn.Linear(64, 32)\n",
    "        self.f5 = nn.Linear(32, num_c)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(x.size())\n",
    "        x = x.float()\n",
    "        \n",
    "        out = self.conv(x)\n",
    "        out = F.relu(out)\n",
    "        # print(out.size())\n",
    "        out = torch.squeeze(out)\n",
    "        # print(out.size())\n",
    "        out = F.max_pool1d(out, 2)\n",
    "        # print(out.size())\n",
    "        out = out.view(-1, 32 * (config.embedding_size - 2))\n",
    "        # print(out.size())\n",
    "        out = F.relu(out)\n",
    "\n",
    "        out = F.relu(self.f1(out))\n",
    "        # print(out.size())\n",
    "#         out = F.relu(self.f2(out))\n",
    "        out = F.relu(self.f3(out))\n",
    "        out = F.relu(self.f4(out))\n",
    "        out = F.relu(self.f5(out))\n",
    "        \n",
    "        # print(out.size())\n",
    "\n",
    "        if num_c == 2:\n",
    "            probs = F.log_softmax(out, dim=1)\n",
    "        else:\n",
    "            probs = out\n",
    "            \n",
    "        # print(probs, probs.size())\n",
    "        # print(probs.size())\n",
    "        classes = torch.max(probs, -1)[1]\n",
    "\n",
    "        return probs, classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model):\n",
    "    viz = Visdom()\n",
    "    step_loss = []\n",
    "    step_precision = []\n",
    "\n",
    "    if num_c == 2:\n",
    "        loss_function = nn.NLLLoss(sample_weight)\n",
    "    else:\n",
    "        # loss_function = nn.CrossEntropyLoss(sample_weight)\n",
    "        loss_function = nn.CrossEntropyLoss(sample_weight)\n",
    "        \n",
    "    optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "\n",
    "    writer = SummaryWriter(log_dir=\"log\")\n",
    "\n",
    "    epoch = 0\n",
    "    step = 0\n",
    "\n",
    "    for epoch in range(1, config.num_epochs + 1):\n",
    "        print(\"================ Epoch: {} ================\".format(epoch))\n",
    "        running_losses = []\n",
    "\n",
    "        for sequences, labels in train_loader:\n",
    "            # Predict\n",
    "            # model.zero_grad()\n",
    "            probs, classes = model(sequences)\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            losses = loss_function(probs, labels)\n",
    "            losses.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Log summary\n",
    "            running_losses.append(losses.data.item())\n",
    "            if step % config.summary_interval == 0:\n",
    "                loss = sum(running_losses) / len(running_losses)\n",
    "                # writer.add_scalar(\"train/loss\", loss, step)\n",
    "                step_loss.append([step, loss])\n",
    "                print(\"step = {}, loss = {:.4f}\".format(step, loss))\n",
    "                running_losses = []\n",
    "\n",
    "            step += 1\n",
    "\n",
    "        # Classification report\n",
    "        for vali in test_loader:\n",
    "            X_test = vali[0]\n",
    "            y_test = vali[1]\n",
    "            probs, y_pred = model(X_test)\n",
    "            # print(y_pred)\n",
    "            if num_c == 5:\n",
    "                target_names = ['non-', 'anger', \"joy\", \"sadness\", \"fear\"]\n",
    "            elif num_c == 4:\n",
    "                target_names = ['anger', \"joy\", \"sadness\", \"fear\"]\n",
    "            elif num_c == 2:\n",
    "                target_names = ['robot', \"human\"]\n",
    "            # logging.info(\"{}\".format(classification_report(y_test, y_pred, target_names=target_names)))\n",
    "            report = classification_report(y_test, y_pred, target_names=target_names, output_dict=True)\n",
    "            print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "            step_precision.append([step, report[\"micro avg\"][\"precision\"]])\n",
    "            break\n",
    "\n",
    "        epoch += 1\n",
    "\n",
    "    # Save\n",
    "    # torch.save(model, \"model/shit{}.pkl\".format(epoch))\n",
    "    \n",
    "\n",
    "    step_loss = np.array(step_loss)\n",
    "    step_precision = np.array(step_precision)\n",
    "    viz.line(X=step_loss[:, 0], Y=step_loss[:, 1])\n",
    "    viz.line(X=step_precision[:, 0], Y=step_precision[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Setting up a new session...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNClassifier(\n",
      "  (conv): Conv2d(2, 64, kernel_size=(3, 300), stride=(1, 1), groups=2)\n",
      "  (f1): Linear(in_features=1536, out_features=128, bias=True)\n",
      "  (f2): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (f3): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (f4): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (f5): Linear(in_features=32, out_features=4, bias=True)\n",
      ")\n",
      "================ Epoch: 1 ================\n",
      "step = 0, loss = 1.4065\n",
      "step = 10, loss = 1.3782\n",
      "step = 20, loss = 1.3449\n",
      "step = 30, loss = 1.2883\n",
      "step = 40, loss = 1.1895\n",
      "step = 50, loss = 1.1214\n",
      "step = 60, loss = 1.0754\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.43      0.71      0.53       234\n",
      "         joy       0.67      0.56      0.61       254\n",
      "     sadness       0.48      0.19      0.27       294\n",
      "        fear       0.54      0.72      0.62       179\n",
      "\n",
      "   micro avg       0.52      0.52      0.52       961\n",
      "   macro avg       0.53      0.55      0.51       961\n",
      "weighted avg       0.53      0.52      0.49       961\n",
      "\n",
      "================ Epoch: 2 ================\n",
      "step = 70, loss = 1.0695\n",
      "step = 80, loss = 1.0037\n",
      "step = 90, loss = 0.9906\n",
      "step = 100, loss = 0.9558\n",
      "step = 110, loss = 0.8979\n",
      "step = 120, loss = 0.8661\n",
      "step = 130, loss = 0.8390\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.55      0.62      0.58       234\n",
      "         joy       0.67      0.63      0.65       254\n",
      "     sadness       0.53      0.35      0.43       294\n",
      "        fear       0.53      0.77      0.62       179\n",
      "\n",
      "   micro avg       0.57      0.57      0.57       961\n",
      "   macro avg       0.57      0.59      0.57       961\n",
      "weighted avg       0.57      0.57      0.56       961\n",
      "\n",
      "================ Epoch: 3 ================\n",
      "step = 140, loss = 0.9059\n",
      "step = 150, loss = 0.8354\n",
      "step = 160, loss = 0.8428\n",
      "step = 170, loss = 0.7944\n",
      "step = 180, loss = 0.7599\n",
      "step = 190, loss = 0.7201\n",
      "step = 200, loss = 0.6969\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.55      0.61      0.58       234\n",
      "         joy       0.68      0.65      0.66       254\n",
      "     sadness       0.61      0.47      0.53       294\n",
      "        fear       0.56      0.74      0.64       179\n",
      "\n",
      "   micro avg       0.60      0.60      0.60       961\n",
      "   macro avg       0.60      0.62      0.60       961\n",
      "weighted avg       0.61      0.60      0.60       961\n",
      "\n",
      "================ Epoch: 4 ================\n",
      "step = 210, loss = 0.7415\n",
      "step = 220, loss = 0.6829\n",
      "step = 230, loss = 0.7062\n",
      "step = 240, loss = 0.6432\n",
      "step = 250, loss = 0.6381\n",
      "step = 260, loss = 0.6012\n",
      "step = 270, loss = 0.6186\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.57      0.54      0.56       234\n",
      "         joy       0.70      0.62      0.66       254\n",
      "     sadness       0.59      0.60      0.59       294\n",
      "        fear       0.60      0.73      0.66       179\n",
      "\n",
      "   micro avg       0.61      0.61      0.61       961\n",
      "   macro avg       0.62      0.62      0.62       961\n",
      "weighted avg       0.62      0.61      0.61       961\n",
      "\n",
      "================ Epoch: 5 ================\n",
      "step = 280, loss = 0.6261\n",
      "step = 290, loss = 0.6089\n",
      "step = 300, loss = 0.6070\n",
      "step = 310, loss = 0.5368\n",
      "step = 320, loss = 0.5796\n",
      "step = 330, loss = 0.4958\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.64      0.43      0.51       234\n",
      "         joy       0.60      0.70      0.65       254\n",
      "     sadness       0.57      0.56      0.57       294\n",
      "        fear       0.59      0.70      0.64       179\n",
      "\n",
      "   micro avg       0.59      0.59      0.59       961\n",
      "   macro avg       0.60      0.60      0.59       961\n",
      "weighted avg       0.60      0.59      0.59       961\n",
      "\n",
      "================ Epoch: 6 ================\n",
      "step = 340, loss = 0.6274\n",
      "step = 350, loss = 0.5273\n",
      "step = 360, loss = 0.5334\n",
      "step = 370, loss = 0.4828\n",
      "step = 380, loss = 0.4816\n",
      "step = 390, loss = 0.5338\n",
      "step = 400, loss = 0.5065\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.60      0.50      0.55       234\n",
      "         joy       0.57      0.77      0.65       254\n",
      "     sadness       0.59      0.47      0.53       294\n",
      "        fear       0.63      0.65      0.64       179\n",
      "\n",
      "   micro avg       0.59      0.59      0.59       961\n",
      "   macro avg       0.60      0.60      0.59       961\n",
      "weighted avg       0.59      0.59      0.59       961\n",
      "\n",
      "================ Epoch: 7 ================\n",
      "step = 410, loss = 0.4600\n",
      "step = 420, loss = 0.4790\n",
      "step = 430, loss = 0.5343\n",
      "step = 440, loss = 0.4201\n",
      "step = 450, loss = 0.4547\n",
      "step = 460, loss = 0.4744\n",
      "step = 470, loss = 0.4003\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.67      0.36      0.47       234\n",
      "         joy       0.62      0.70      0.66       254\n",
      "     sadness       0.56      0.52      0.54       294\n",
      "        fear       0.49      0.77      0.60       179\n",
      "\n",
      "   micro avg       0.57      0.57      0.57       961\n",
      "   macro avg       0.59      0.59      0.57       961\n",
      "weighted avg       0.59      0.57      0.56       961\n",
      "\n",
      "================ Epoch: 8 ================\n",
      "step = 480, loss = 0.4381\n",
      "step = 490, loss = 0.4174\n",
      "step = 500, loss = 0.4402\n",
      "step = 510, loss = 0.3969\n",
      "step = 520, loss = 0.3363\n",
      "step = 530, loss = 0.2619\n",
      "step = 540, loss = 0.2361\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.58      0.44      0.50       234\n",
      "         joy       0.62      0.67      0.64       254\n",
      "     sadness       0.53      0.58      0.55       294\n",
      "        fear       0.66      0.66      0.66       179\n",
      "\n",
      "   micro avg       0.59      0.59      0.59       961\n",
      "   macro avg       0.59      0.59      0.59       961\n",
      "weighted avg       0.59      0.59      0.58       961\n",
      "\n",
      "================ Epoch: 9 ================\n",
      "step = 550, loss = 0.2468\n",
      "step = 560, loss = 0.2175\n",
      "step = 570, loss = 0.2823\n",
      "step = 580, loss = 0.2765\n",
      "step = 590, loss = 0.2337\n",
      "step = 600, loss = 0.1967\n",
      "step = 610, loss = 0.2007\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.63      0.31      0.41       234\n",
      "         joy       0.55      0.73      0.63       254\n",
      "     sadness       0.50      0.64      0.56       294\n",
      "        fear       0.78      0.56      0.65       179\n",
      "\n",
      "   micro avg       0.57      0.57      0.57       961\n",
      "   macro avg       0.61      0.56      0.56       961\n",
      "weighted avg       0.60      0.57      0.56       961\n",
      "\n",
      "================ Epoch: 10 ================\n",
      "step = 620, loss = 0.1945\n",
      "step = 630, loss = 0.1616\n",
      "step = 640, loss = 0.2127\n",
      "step = 650, loss = 0.2032\n",
      "step = 660, loss = 0.1848\n",
      "step = 670, loss = 0.1761\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.59      0.31      0.40       234\n",
      "         joy       0.46      0.82      0.59       254\n",
      "     sadness       0.51      0.47      0.49       294\n",
      "        fear       0.82      0.53      0.64       179\n",
      "\n",
      "   micro avg       0.53      0.53      0.53       961\n",
      "   macro avg       0.59      0.53      0.53       961\n",
      "weighted avg       0.57      0.53      0.52       961\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = CNNClassifier()\n",
    "print(model)\n",
    "train(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
