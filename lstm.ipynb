{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "     \n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.i2o(combined)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return Variable(torch.zeros(1, self.hidden_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded succeed\n",
      "加载词向量中 ...\n",
      "加载词完成！一共 15个词\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "sizes must be non-negative",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-292773fc765e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMyDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-292773fc765e>\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     44\u001b[0m                         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m                 \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mline_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-292773fc765e>\u001b[0m in \u001b[0;36mline_to_tensor\u001b[0;34m(self, line)\u001b[0m\n\u001b[1;32m     58\u001b[0m                 \u001b[0mlist_vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlist_vec\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: sizes must be non-negative"
     ]
    }
   ],
   "source": [
    "from thulac import thulac\n",
    "thu = thulac(seg_only=True)\n",
    "\n",
    "class MyDataset():\n",
    "        \n",
    "    def __init__(self):\n",
    "        self.data = {0: [], 1: [], 2: [], 3: [], 4: []}\n",
    "        self.word_vec = self.load_word_vector()\n",
    "        self.vector_dim = 300\n",
    "        \n",
    "    def load_word_vector(self, in_name='data/sgns.financial.word'):\n",
    "        \"\"\"\n",
    "        加载ACL2018词向量\n",
    "        \"\"\"\n",
    "        word_vec = {}\n",
    "        print('加载词向量中 ...')\n",
    "        for i, line in enumerate(in_name):\n",
    "    #         if i <= 10:\n",
    "    #             continue\n",
    "            if i > 250000:\n",
    "                break\n",
    "            words = line.strip().split(' ')\n",
    "            word = words[0]\n",
    "            word_vec[word] = Tensor([float(num) for num in words[1:]])\n",
    "    #         except UnicodeDecodeError:\n",
    "    #             print(\"编码问题，行 {}\".format(i))\n",
    "        print('加载词完成！一共 {}个词'.format(len(word_vec)))\n",
    "        return word_vec\n",
    "\n",
    "    def load(self):\n",
    "        \"\"\"\n",
    "        加载原始文本\n",
    "        \"\"\"\n",
    "        for in_name in glob.glob('data/labelled/*.txt'):\n",
    "            for i, line in enumerate(open(in_name)):\n",
    "                if line.strip() == '':\n",
    "                    continue\n",
    "                label = line.split('\\t')[0]\n",
    "                # 1234：四种情绪，-：没有情绪，x：不确定\n",
    "                if label in ['1', '2', '3', '4', '-']:\n",
    "                    if label == '-' or label == 'x':\n",
    "                        y = int('0')\n",
    "                    else:\n",
    "                        y = int(label)\n",
    "\n",
    "                X = self.line_to_tensor(line.split('\\t')[1])\n",
    "                self.data[y].append(X)\n",
    "                       \n",
    "    def line_to_tensor(self, line):\n",
    "        \"\"\"\n",
    "        一句话转向量\n",
    "        \"\"\"\n",
    "        list_vec = []\n",
    "        for w in thu.cut(line): # 对分词结果进行处理\n",
    "            w = w[0]\n",
    "            if w in self.word_vec:\n",
    "                list_vec.append(self.word_vec[w])\n",
    "        tensor = torch.as_tensor([list_vec])\n",
    "        return tensor\n",
    "    \n",
    "\n",
    "dataset = MyDataset()\n",
    "dataset.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 设置参数\n",
    "input_size, hidden_size, output_size = 300, 100, 5\n",
    "learning_rate = 1e-5\n",
    "EPOCH = 5000\n",
    "\n",
    "# 定义函数\n",
    "mod = MyNet(input_size, hidden_size, output_size)\n",
    "\n",
    "# 定义损失函数\n",
    "loss_fn = nn.MSELoss(size_average=False)\n",
    "\n",
    "# 优化器\n",
    "optimizer = torch.optim.SGD(mod.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11615.4599609375\n",
      "8750.8349609375\n",
      "8617.62109375\n",
      "8526.04296875\n",
      "8466.537109375\n",
      "8412.2197265625\n",
      "8342.642578125\n",
      "8262.72265625\n",
      "8200.2548828125\n",
      "8079.6328125\n"
     ]
    }
   ],
   "source": [
    "## 开始训练 ##\n",
    "for t in range(EPOCH):\n",
    "    \n",
    "    # 向前传播\n",
    "    y_pred = mod(x_data)\n",
    "    \n",
    "    # 计算损失\n",
    "    loss = loss_fn(y_pred, y_data)\n",
    "    # 显示损失\n",
    "    if t % 500 == 0:\n",
    "        print(loss.data.item())\n",
    "    \n",
    "    # 在我们进行梯度更新之前，\n",
    "    # 先使用optimier对象提供的清除已经积累的梯度。\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # 计算梯度\n",
    "    loss.backward()\n",
    "    \n",
    "    # 更新梯度\n",
    "    optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
